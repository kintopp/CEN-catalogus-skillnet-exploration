{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3dbc96",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Background</a></span></li><li><span><a href=\"#Introduction-to-the-jupyter-notebook\" data-toc-modified-id=\"Introduction-to-the-jupyter-notebook-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Introduction to the jupyter notebook</a></span></li><li><span><a href=\"#How-to-use-this-notebook-(for-beginners)\" data-toc-modified-id=\"How-to-use-this-notebook-(for-beginners)-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>How to use this notebook (for beginners)</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Tests</a></span></li></ul></li><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import libraries</a></span></li><li><span><a href=\"#Import-files-and-prepare-data\" data-toc-modified-id=\"Import-files-and-prepare-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Import files and prepare data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-paths-to-data\" data-toc-modified-id=\"Set-paths-to-data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Set paths to data</a></span></li><li><span><a href=\"#Read-files\" data-toc-modified-id=\"Read-files-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Read files</a></span></li><li><span><a href=\"#Prepare-all-persons-format\" data-toc-modified-id=\"Prepare-all-persons-format-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Prepare all persons format</a></span></li><li><span><a href=\"#Combine-metadata-from-persons-and-letters-(processing)\" data-toc-modified-id=\"Combine-metadata-from-persons-and-letters-(processing)-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Combine metadata from persons and letters (processing)</a></span></li></ul></li><li><span><a href=\"#CEN-Letters-overview\" data-toc-modified-id=\"CEN-Letters-overview-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>CEN Letters overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#CEN-dataset-description\" data-toc-modified-id=\"CEN-dataset-description-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>CEN dataset description</a></span></li><li><span><a href=\"#How-many-letters?\" data-toc-modified-id=\"How-many-letters?-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>How many letters?</a></span></li><li><span><a href=\"#From-which-years?\" data-toc-modified-id=\"From-which-years?-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>From which years?</a></span></li><li><span><a href=\"#From-which-archives?\" data-toc-modified-id=\"From-which-archives?-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>From which archives?</a></span></li></ul></li><li><span><a href=\"#CEN-Persons-overview\" data-toc-modified-id=\"CEN-Persons-overview-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CEN Persons overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unique-persons-(entities)\" data-toc-modified-id=\"Unique-persons-(entities)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Unique persons (entities)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Original-data-(sender/receiver)\" data-toc-modified-id=\"Original-data-(sender/receiver)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Original data (sender/receiver)</a></span></li><li><span><a href=\"#Types-of-unique-entities\" data-toc-modified-id=\"Types-of-unique-entities-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Types of unique entities</a></span></li></ul></li><li><span><a href=\"#Letters-per-person/entity\" data-toc-modified-id=\"Letters-per-person/entity-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Letters per person/entity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Letters-per-unique-senders\" data-toc-modified-id=\"Letters-per-unique-senders-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Letters per unique senders</a></span></li><li><span><a href=\"#Letters-per-unique-receivers\" data-toc-modified-id=\"Letters-per-unique-receivers-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Letters per unique receivers</a></span></li><li><span><a href=\"#Letters-per-unique-entity-(person,-organization,-etc.)\" data-toc-modified-id=\"Letters-per-unique-entity-(person,-organization,-etc.)-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Letters per unique entity (person, organization, etc.)</a></span></li></ul></li><li><span><a href=\"#Relations\" data-toc-modified-id=\"Relations-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Relations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choose-your-person-of-interest\" data-toc-modified-id=\"Choose-your-person-of-interest-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Choose your person of interest</a></span></li><li><span><a href=\"#First-degree-network\" data-toc-modified-id=\"First-degree-network-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>First-degree network</a></span></li><li><span><a href=\"#Second-degree-network\" data-toc-modified-id=\"Second-degree-network-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Second-degree network</a></span></li><li><span><a href=\"#Generate-network-data\" data-toc-modified-id=\"Generate-network-data-5.3.4\"><span class=\"toc-item-num\">5.3.4&nbsp;&nbsp;</span>Generate network data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Nodes\" data-toc-modified-id=\"Nodes-5.3.4.1\"><span class=\"toc-item-num\">5.3.4.1&nbsp;&nbsp;</span>Nodes</a></span></li><li><span><a href=\"#Edges\" data-toc-modified-id=\"Edges-5.3.4.2\"><span class=\"toc-item-num\">5.3.4.2&nbsp;&nbsp;</span>Edges</a></span></li></ul></li></ul></li><li><span><a href=\"#Other\" data-toc-modified-id=\"Other-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Other</a></span><ul class=\"toc-item\"><li><span><a href=\"#Women\" data-toc-modified-id=\"Women-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Women</a></span></li><li><span><a href=\"#Roles\" data-toc-modified-id=\"Roles-5.4.2\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>Roles</a></span></li><li><span><a href=\"#Ages\" data-toc-modified-id=\"Ages-5.4.3\"><span class=\"toc-item-num\">5.4.3&nbsp;&nbsp;</span>Ages</a></span></li></ul></li></ul></li><li><span><a href=\"#Exports\" data-toc-modified-id=\"Exports-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Exports</a></span><ul class=\"toc-item\"><li><span><a href=\"#All-letters\" data-toc-modified-id=\"All-letters-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>All letters</a></span></li><li><span><a href=\"#Nodes-and-Edges\" data-toc-modified-id=\"Nodes-and-Edges-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Nodes and Edges</a></span></li><li><span><a href=\"#Letters-by/to-person-of-interest-(first-degree)\" data-toc-modified-id=\"Letters-by/to-person-of-interest-(first-degree)-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Letters by/to person of interest (first degree)</a></span></li><li><span><a href=\"#Letters-by/to-person-of-interest-(second-degree)\" data-toc-modified-id=\"Letters-by/to-person-of-interest-(second-degree)-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Letters by/to person of interest (second degree)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94849123",
   "metadata": {
    "id": "94849123"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293a280",
   "metadata": {
    "id": "f293a280"
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb61de0",
   "metadata": {
    "id": "1fb61de0"
   },
   "source": [
    "This jupyter notebook was created in the context of the SKILLNET project (skillnet.nl/). \n",
    "\n",
    "The SKILLNET project (https://skillnet.nl/) is a European Research Council (ERC)-funded project led by Dirk van Miert (https://orcid.org/0000-0002-5460-4075) at Utrecht University. This project investigates the ideals of sharing knowledge as a legacy of a bottom-up social network of scholars and scientists from the Early Modern period.\n",
    "\n",
    "One of the most important datasets curated during this project is the Catalogus Epistolarum Neerlandicarum (CEN). The CEN is the Dutch national letter catalog, which aggregates letter metadata from different universities in the Netherlands and from the National Library of the Netherlands (KB), among others, since the years 1980's to the present. The curated version of a slice of the entire data (approx. 10% of the entire CEN) is offered in this dataset. It includes the letters between 1270 and 1820 plus some undated letters, which is of interest for the study of Early Modern correspondence.\n",
    "\n",
    "The complete description and the dataset is available in Dataverse (https://dataverse.nl/dataverse/skillnet). The specific dataset that is used by this notebook can be cited with this DOI (add always version number to the citation): https://doi.org/10.34894/G8XQI0.\n",
    "\n",
    "This jupyter notebook is part of a Github repository which is deposited in Zenodo and can be cited with this DOI: https://doi.org/10.5281/zenodo.7309977. The original repository is here: https://github.com/lilimelgar/CEN-catalogus-skillnet-exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee7e7f",
   "metadata": {
    "id": "ccee7e7f"
   },
   "source": [
    "## Introduction to the jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5eab3",
   "metadata": {
    "id": "67f5eab3"
   },
   "source": [
    "This repository includes the jupyter notebook that was created for exploring the CEN curated dataset by SKILLNET. \n",
    "\n",
    "Jupyter notebooks (https://jupyter.org/) are web environments where code can be written and executed, and it is not only code, but it also allows the integration of narrative text. Jupyter notebooks can be ran in your own computer by installing them (for example, using the Anaconda distribution (https://www.anaconda.com/products/distribution)). However, to facilitate its use by people who are not interested in installing the programs, or to avoid conflicting versioning of these programs, we have used \"My binder\" (https://mybinder.readthedocs.io/en/latest/#) to pack the repository, which is ready to be used online here: https://edu.nl/bn93d (1).\n",
    "\n",
    "This notebook provides access to the latest version of the CEN catalog slice (a exact copy is deposited in Dataverse, link above), and offers basic functionalities to query and visualize the data. As a researcher interested in the letters metadata of this period you can use this notebook for:\n",
    "\n",
    "- Getting an overview of how many letters are in the dataset, in total and per year\n",
    "- Getting an overview of the correspondents (persons) in the dataset, and the amount of letters they exchanged\n",
    "- Generating a first-degree network for a person of interest\n",
    "- Generating a second-degree network for a person of interest\n",
    "- Downloading the \"nodes\" and \"edges\" files necessary for network analysis in other tools (e.g., Gephi (https://gephi.org/))\n",
    "- Understanding the challenges and solutions we used to clean the original dataset\n",
    "\n",
    "The data overview facilitated by the notebook is offered to researchers as a basic way to see \"what's in the data\", but for more complex analyses it is recommended to download the latest version of the entire dataset from Dataverse and perform their own analyzes.\n",
    "\n",
    "This jupyter notebook is part of a Github repository which is deposited in Zenodo and can be cited with this DOI: https://doi.org/10.5281/zenodo.7309977. The original repository is here: https://github.com/lilimelgar/CEN-catalogus-skillnet-exploration.\n",
    "\n",
    "\n",
    "---\n",
    "- (1) Original (non-shortened) URL: https://mybinder.org/v2/gh/lilimelgar/CEN-catalogus-exploration/HEAD?labpath=%2Fsrc%2FCEN_SKILLNETslice_overview.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d1373",
   "metadata": {},
   "source": [
    "## How to use this notebook (for beginners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c1b1e2",
   "metadata": {},
   "source": [
    "The notebook has cells with python code that can be executed, and other cells with explanatory text written in Markdown (which is a basic text editing language) (https://en.wikipedia.org/wiki/Markdown). Below a cell with code there is often a cell with output. \n",
    "\n",
    "This notebook is meant to work without having to write any code yourself in order to get those outputs. If you click in the main menu above the option \"Cell -> Run all\", the notebook will produce results and visualizations, which are explained in the explanatory text cells. However, there is also the option to do simple manipulations: for example, instead of getting the first 20 correspondents, you can decide you want to see 50. In that case, you need to change that value in the cell. When this is the case, the cell has a header above named \"Your input here\".\n",
    "\n",
    "If you want to read the notebook step by step, you can then:\n",
    "- One option is that you run one cell at the time. However, Section 2 and 3 are mostly preparatory (you don't get any interesting output from them), thus, you can skip those, but you will have to execute them. For that purpose put your cursor in Section 4: CEN Letters overview. Click on the menu \"Cell -> Run all above\". This is to execute all the cells that download and prepare the data.\n",
    "- Then you can go cell by cell reading the explanations and executing the code to get the outputs.\n",
    "- When you see a header \"Give input here\" you can then change the values according to your wishes.\n",
    "- Some cells are meant to facilitate exporting the data. Those cells are \"commented\". This means that they are not executed unless you decide so. Otherwise, every time that you run the notebook you will get many files downloaded to your computer without you having decided that. The symbol to distinguish a comment is the dash: #. If a cell is commented, all the code inside it has a dash or two dashes. In order to uncomment it, you can select all the content of the cell and press \"CTRL\" + /.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d8f7a",
   "metadata": {
    "id": "f09d8f7a"
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188878cb",
   "metadata": {
    "id": "188878cb"
   },
   "source": [
    "- Test1: This is a text cell (see the type above: \"Markdown\").\n",
    "- Click on the button \"Run\" above and see what happens (nothing should happen). \n",
    "- Now change the type of cell to \"Code\" and run it again (now you should get an error)... change the type back to \"Markdown\".\n",
    "\n",
    "This is hard to read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SKfvrPOr8cE_",
   "metadata": {
    "id": "SKfvrPOr8cE_"
   },
   "source": [
    "Here I can write anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RcVGPeVH8eN7",
   "metadata": {
    "id": "RcVGPeVH8eN7"
   },
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bccec8",
   "metadata": {
    "id": "82bccec8"
   },
   "source": [
    "Test2: **Your input here!**\n",
    "Remember that \"Your input here!\" means that you can enter your own values or parameters in the code cell below this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba9cd1",
   "metadata": {
    "id": "e1ba9cd1"
   },
   "outputs": [],
   "source": [
    "# This is a test --> the dash indicates this is a comment within a code cell (check the type above)\n",
    "# Please type your name between the quotation marks replacing the text \"Enter your name here\". Then execute the cell (you can click on \"Run\" in the menu above, or use the cell buttons)\n",
    "my_name = 'Dirk'\n",
    "print(f'Hello {my_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0902ad8",
   "metadata": {
    "id": "a0902ad8"
   },
   "source": [
    "Test3: Insert a cell of the type \"Markdown\" and add some test comments there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2aff0c",
   "metadata": {
    "id": "4c2aff0c"
   },
   "source": [
    "Now two sections follow (2 and 3) which have the code that imports the libraries and files necessary for this notebook. Besides code, there is not much to see here since there are no outputs. If you want to skip these parts quickly, go to section 4 (CEN letters overview) and click on Cell -> Run All Above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6a65b",
   "metadata": {
    "id": "42b6a65b"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d38c9",
   "metadata": {
    "id": "7f9d38c9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#source (https://data36.com/plot-histogram-python-pandas/)\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:65% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# to add timestamp to file names\n",
    "import time\n",
    "\n",
    "# import requests\n",
    "# import io\n",
    "# import zipfile\n",
    "\n",
    "# to visualize networks\n",
    "import networkx as nx\n",
    "# import scipy as sp\n",
    "# import scipy.optimize  # call as sp.optimize\n",
    "\n",
    "\n",
    "# import os.path to add paths to files\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d80c14",
   "metadata": {
    "id": "53d80c14"
   },
   "source": [
    "# Import files and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def997cd",
   "metadata": {},
   "source": [
    "## Set paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the main data folder in the repository\n",
    "data_directory = os.path.abspath(os.path.join('..', 'data'))\n",
    "# path to the sub-folder in the repository where the raw (unprocessed) data is located\n",
    "data_raw_directory = os.path.join(data_directory, 'raw')\n",
    "# path to the sub-folder in the repository where the processed data is located\n",
    "data_processed_directory = os.path.join(data_directory, 'processed')\n",
    "# path to the sub-folder in the repository to store temporary data files\n",
    "data_temp_directory = os.path.join(data_directory, 'temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dc2f4",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b86749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read letters file\n",
    "letters_path = os.path.join(data_raw_directory, 'letters_CEN_1200to1820_cleanedSkillnet.csv')\n",
    "\n",
    "cen_letters_imp = pd.read_csv(letters_path, sep = \",\", index_col=False, engine='python')\n",
    "\n",
    "# convert datatypes and fill in empty values\n",
    "\n",
    "# convert and fill in depending on original data type\n",
    "cen_letters_imp_columns = cen_letters_imp.columns\n",
    "for column in cen_letters_imp_columns:\n",
    "    dataType = cen_letters_imp.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        cen_letters_imp[column] = cen_letters_imp[column].fillna(0.0)\n",
    "        cen_letters_imp[column] = cen_letters_imp[column].astype(int)\n",
    "    if dataType == object:\n",
    "        cen_letters_imp[column] = cen_letters_imp[column].fillna('null')\n",
    "        cen_letters_imp[column] = cen_letters_imp[column].astype(str)\n",
    "        \n",
    "# # Handle exceptions\n",
    "# cen_letters_imp['emloLetterId_inferredSK'] = cen_letters_imp['emloLetterId_inferredSK'].astype(str)\n",
    "\n",
    "# make a copy\n",
    "cen_letters = cen_letters_imp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "cen_letters.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read persons file\n",
    "persons_path = os.path.join(data_raw_directory, 'persons_CEN_1200to1800_cleanedSkillnet.csv')\n",
    "\n",
    "cen_persons_imp = pd.read_csv(persons_path, sep = \",\", index_col=False, engine='python')\n",
    "\n",
    "# convert datatypes and fill in empty values\n",
    "\n",
    "# convert and fill in depending on original data type\n",
    "cen_persons_imp_columns = cen_persons_imp.columns\n",
    "for column in cen_persons_imp_columns:\n",
    "    dataType = cen_persons_imp.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        cen_persons_imp[column] = cen_persons_imp[column].fillna(0.0)\n",
    "        cen_persons_imp[column] = cen_persons_imp[column].astype(int)\n",
    "    if dataType == object:\n",
    "        cen_persons_imp[column] = cen_persons_imp[column].fillna('null')\n",
    "        cen_persons_imp[column] = cen_persons_imp[column].astype(str)\n",
    "        \n",
    "# make a copy\n",
    "cen_persons = cen_persons_imp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ac1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cen_letters_imp.info()\n",
    "cen_persons.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e45c1",
   "metadata": {
    "id": "c59e45c1"
   },
   "source": [
    "## Prepare all persons format\n",
    "Here the Letters (where senders/receivers are in different columns) gets converted to the \"all persons\" format where every sender/receiver is in a unique row (with Letter Ids). This is to connect the persons to the letters (via the SKpersonId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c3f99",
   "metadata": {
    "id": "148c3f99"
   },
   "outputs": [],
   "source": [
    "# Select only the columns that will be used for the counting of letters\n",
    "\n",
    "cen_allPersons_t0 = cen_letters[['SKletterId',\n",
    "'AFZENDER_OriginalCEN',\n",
    "'SKpersonId_sender',\n",
    "'personStrId_sender',\n",
    "'isUncertainAfzender',\n",
    "'ONTVANGER_OriginalCEN',\n",
    "'SKpersonId_receiver',\n",
    "'personStrId_receiver',\n",
    "'isUncertainOntvanger',\n",
    "'quantity_CleanedSk'\n",
    "]].copy()\n",
    "\n",
    "cen_allPersons_t1 = cen_allPersons_t0.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8b2a8",
   "metadata": {
    "id": "83d8b2a8"
   },
   "outputs": [],
   "source": [
    "# Create Df for Afzenders\n",
    "dfAfzenders = cen_allPersons_t1.copy()\n",
    "# drop columns that belong to \"ontvanger\"\n",
    "dfAfzendersOnly = dfAfzenders.drop(['SKpersonId_receiver', 'personStrId_receiver', 'ONTVANGER_OriginalCEN', 'isUncertainOntvanger'], axis=1)\n",
    "# create Id for the person in relation to role (to bring persons data back and forth)\n",
    "dfAfzendersOnly['SKletterIdWithRole'] = dfAfzendersOnly['SKletterId'] + '_afzender'\n",
    "# # rename columns\n",
    "dfAfzendersOnly.rename(columns={'SKpersonId_sender':'SKpersonId', 'personStrId_sender':'personStrId', 'AFZENDER_OriginalCEN':'AFZENDER_ONTVANGER_OriginalCEN', 'isUncertainAfzender':'isUncertainCorrespondent'},inplace=True)\n",
    "# dfAfzendersOnly.info()\n",
    "\n",
    "# Create Df for Ontvangers\n",
    "dfOntvangers = cen_allPersons_t1.copy()\n",
    "# drop columns that belong to \"afzender\"\n",
    "dfOntvangersOnly = dfOntvangers.drop(['SKpersonId_sender', 'personStrId_sender', 'AFZENDER_OriginalCEN', 'isUncertainAfzender'], axis=1)\n",
    "# create Id for the user in relation to role (to bring persons data back and forth)\n",
    "dfOntvangersOnly['SKletterIdWithRole'] = dfOntvangersOnly['SKletterId'] + '_ontvanger'\n",
    "# rename columns\n",
    "dfOntvangersOnly.rename(columns={'SKpersonId_receiver':'SKpersonId', 'personStrId_receiver':'personStrId', 'ONTVANGER_OriginalCEN':'AFZENDER_ONTVANGER_OriginalCEN', 'isUncertainOntvanger':'isUncertainCorrespondent'},inplace=True)\n",
    "# dfOntvangersOnly.info()\n",
    "\n",
    "# merge the two sets\n",
    "cen_allPersons_t2 = pd.concat([dfAfzendersOnly,dfOntvangersOnly])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa99865",
   "metadata": {
    "id": "0aa99865"
   },
   "outputs": [],
   "source": [
    "cen_allPersons_t2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c4f1e",
   "metadata": {
    "id": "0a0c4f1e"
   },
   "outputs": [],
   "source": [
    "# if it's all Ok, make a copy:\n",
    "cen_allPersons_t3 = cen_allPersons_t2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5eff6",
   "metadata": {
    "id": "49c5eff6"
   },
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "\n",
    "cen_allPersons_t3_columns = cen_allPersons_t3.columns\n",
    "\n",
    "for column in cen_allPersons_t3_columns:\n",
    "    dataType = cen_allPersons_t3.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        cen_allPersons_t3[column] = cen_allPersons_t3[column].fillna(0.0)\n",
    "        cen_allPersons_t3[column] = cen_allPersons_t3[column].astype(int)\n",
    "    if dataType == object:\n",
    "        cen_allPersons_t3[column] = cen_allPersons_t3[column].fillna('null')\n",
    "\n",
    "# make a copy:\n",
    "cen_allPersons = cen_allPersons_t3.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba757d26",
   "metadata": {
    "id": "ba757d26"
   },
   "outputs": [],
   "source": [
    "cen_allPersons.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c396d",
   "metadata": {
    "id": "229c396d"
   },
   "source": [
    "## Combine metadata from persons and letters (processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddda99",
   "metadata": {
    "id": "55ddda99"
   },
   "outputs": [],
   "source": [
    "# combine letters metadata with persons metadata (uniquePersons)\n",
    "allPersons_letters_t0 = cen_allPersons.merge(cen_persons, on='SKpersonId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08017e2",
   "metadata": {
    "id": "e08017e2"
   },
   "outputs": [],
   "source": [
    "# allPersons_letters_t0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b50a90",
   "metadata": {
    "id": "88b50a90"
   },
   "outputs": [],
   "source": [
    "# drop non-relevant columns generated during merge\n",
    "allPersons_letters_t1 = allPersons_letters_t0.drop(['personStrId_y', 'AFZENDER_ONTVANGER_OriginalCEN_y'], axis=1)\n",
    "# rename columns\n",
    "allPersons_letters_t1.rename(columns={\"personStrId_x\":\"personStrId\", \"AFZENDER_ONTVANGER_OriginalCEN_x\": \"AFZENDER_ONTVANGER_OriginalCEN\"},inplace=True)\n",
    "# allPersonsB_t01.head(5)\n",
    "allPersons_letters = allPersons_letters_t1.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf7ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "allPersons_letters.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eihLjuXKypT",
   "metadata": {
    "id": "5eihLjuXKypT"
   },
   "outputs": [],
   "source": [
    "# add date of last update\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "last_update = timestr\n",
    "last_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d705d",
   "metadata": {
    "id": "4d9d705d"
   },
   "source": [
    "# CEN Letters overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c54bfd",
   "metadata": {
    "id": "b1c54bfd"
   },
   "source": [
    "This section includes explanations about the CEN dataset and its structure, and the counts of letters, letters per year, and records per archive. It also walks you through some of the challenges of the cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b7f7b",
   "metadata": {
    "id": "535b7f7b"
   },
   "source": [
    "## CEN dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62fd6d3",
   "metadata": {
    "id": "a62fd6d3"
   },
   "source": [
    "The CEN dataset is in a tabular format (it is a table with rows and columns where the columns represent the metadata fields (e.g., title, language), and the rows represent the records. The data is in csv format (comma separated).\n",
    "\n",
    "How many records(rows)? Each row in the CEN dataset represents a \"metadata record\", which describes either one or multiple letters. Thus, we cannot say that the number of rows is equal to the number of letters (more on this later), but here below you will see how many rows = records are in the dataset. Remember that we are referring to the slice of the bigger CEN that was cleaned during the SKILLNET project, as described in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1de4c8",
   "metadata": {
    "id": "bf1de4c8"
   },
   "outputs": [],
   "source": [
    "# rows in the CEN letters dataset\n",
    "number_rows = len(cen_letters.index)\n",
    "number_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb113c19",
   "metadata": {
    "id": "eb113c19"
   },
   "source": [
    "How many columns? (i.e., what metadata is available for the letters?)\n",
    "\n",
    "The original columns from the CEN dataset (as they were given from OCLC/KB) are kept in the dataset. The initial data dump had these columns:\n",
    "\n",
    "- seqnr,ID,isbd,Jaar,Taal,Afzender,Signatuur,Titel,Aantal,Datering,Ontvanger,Plaats,X1700,Herkomst,Annotatie\n",
    "\n",
    "In the cleaning process undertaken by Skillnet, we created two types of files:\n",
    "- One file for the letter's metadata (which we describe in this section)\n",
    "- One file for the person's metadata (which we describe in the next section)\n",
    "\n",
    "In both cases, additional columns were added, which were necessary for splitting the data in a way that was structured and normalized to facilitate consultation and network analysis research. In any case, the original data was preserved by having together the original column and the \"cleaned\" column(s) next to each other. For example, the original column \"Afzender\" was renamed as 'AFZENDER_OriginalCEN' and, next to it, there are the columns added by Skillnet: \"SKpersonId_sender\" (a person Id that we added) and \"personStrId_sender\" (the normalized form of the name with dates of birth, death and/or floriat) and \"isUncertainAfzender\" (to register uncertainty marks). More examples follow next.\n",
    "\n",
    "While the original dataset had 14 columns, the cleaned version has more columns including the original data columns, the cleaned data columns, and extra columns for control purposes and to register uncertainty in a controlled way (e.g., 'isYearRange', 'isDateringUncertain')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82d7c4",
   "metadata": {
    "id": "af82d7c4"
   },
   "outputs": [],
   "source": [
    "# columns in the CEN letters dataset\n",
    "number_columns = len(cen_letters.columns)\n",
    "number_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7b011",
   "metadata": {
    "id": "2fc7b011"
   },
   "outputs": [],
   "source": [
    "# Column names\n",
    "(cen_letters.columns).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972209a",
   "metadata": {
    "id": "d972209a"
   },
   "source": [
    "## How many letters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde49fe",
   "metadata": {
    "id": "9cde49fe"
   },
   "source": [
    "The original dataset has a column called \"aantal\" (here named as \"AANTAL_OriginalCEN\") in which cataloguers use, among other terms, the words \"brief\" and \"brieven\" to refer to either one single letter or to a bunch of letters, respectively. The original metadata had also notes added to these descriptions (e.g., digitized, number of folios, etc.)\n",
    "Here below you can take a look to how the original metadata looked like before cleaning this column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe298",
   "metadata": {
    "id": "fa7fe298"
   },
   "outputs": [],
   "source": [
    "# unique values in the original CEN metadata for \"aantal\"\n",
    "letters_amount_df = cen_letters[['AANTAL_OriginalCEN', 'quantity_BinaryTypeSk','quantity_CleanedSk', 'quantity_DocumentTypeSk']]\n",
    "letters_original_amount_count = letters_amount_df.AANTAL_OriginalCEN.value_counts()\n",
    "letters_original_amount_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8ad0c",
   "metadata": {
    "id": "6de8ad0c"
   },
   "source": [
    "Thus, at SKILLNET we cleaned the \"aantal\" column by dividing the original information into two columns:\n",
    "\n",
    "a) The type (\"brief\" or \"brieven\"), the column is named \"quantity_BinaryTypeSk\"\n",
    "  - We classified as \"brieven\" all records that had a number bigger than \"1\" accompanied with the word \"brieven\". In some cases, when there was a higher number than 1 and the word was \"brief\", we also changed this to \"brieven\". We also took into consideration the \"date\" field to decide whether a record was a \"brief\" or \"brieven\", checking if there was a single year (then it would be \"brief\") or multiple years or a year range (in that case it will be \"brieven\"). The title of the record also had some words that could indicate if it was a \"brief\" or multiple letters.\n",
    "\n",
    "b) The amount (using 1 for \"brief\", and the number of \"brieven\")\n",
    "  - We added \"1\" to all records that were classified as \"brief\". For \"brieven\" we added the number of letters if this one was indicated. We judged every case in which there was a number accompanied by another word than \"brieven\" (e.g., 33 dozen) to decide which number to add. In this case of the 33 boxes, we classified the record as \"brieven\" but added a \"0\" to the amount of brieven, because it is not possible to know how many letters are in those 33 boxes. Zero, then, represents \"undetermined\".\n",
    "  \n",
    "Here we see the approximate number of RECORDS in the CEN dataset that was curated by SKILLNET per type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836da99",
   "metadata": {
    "id": "9836da99"
   },
   "outputs": [],
   "source": [
    "# number of records (=rows) that contain either one single letter or multiple letters\n",
    "# excluding the \"brieven\" which amount was \"undetermined\"\n",
    "letters_amount_type_counts = letters_amount_df.quantity_BinaryTypeSk.value_counts()\n",
    "letters_amount_type_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e2f7c8",
   "metadata": {
    "id": "78e2f7c8"
   },
   "source": [
    "If we sum the values, we get an approximate number of letters here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb117e4",
   "metadata": {
    "id": "0eb117e4"
   },
   "outputs": [],
   "source": [
    "# approximate number of letters in the CEN dataset (years 1200 to 1820)\n",
    "# summing the number of \"brieven\" in the column \"quantity_CleanedSk\"\n",
    "letters_amount_count = letters_amount_df.quantity_CleanedSk.sum()\n",
    "letters_amount_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9acac6d",
   "metadata": {
    "id": "c9acac6d"
   },
   "source": [
    "But it will never be possible to know the exact amount of letters in the CEN dataset, because there are rows of the type \"brieven\" which don't include the specific number of letters, but descriptions of undertermined bunches, such as \"1 doos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca995a",
   "metadata": {
    "id": "a9ca995a"
   },
   "outputs": [],
   "source": [
    "# Here we can see how many records of the type \"brieven\" had an \"undetermined\" value\n",
    "brieven_undetermined = letters_amount_df[(letters_amount_df.quantity_BinaryTypeSk == 'brieven') & (letters_amount_df.quantity_CleanedSk == 0)]\n",
    "len(brieven_undetermined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a6737",
   "metadata": {
    "id": "f00a6737"
   },
   "outputs": [],
   "source": [
    "# Here we can see which types of containers hold those letters:\n",
    "containers_brieven = brieven_undetermined.AANTAL_OriginalCEN.value_counts()\n",
    "containers_brieven"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b752df",
   "metadata": {
    "id": "00b752df"
   },
   "source": [
    "**Your input here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b7f2c",
   "metadata": {
    "id": "780b7f2c"
   },
   "outputs": [],
   "source": [
    "# Try it yourself: find, for example, how many times cataloguers used words for \"box\" in the description:\n",
    "search1_df = letters_amount_df[letters_amount_df.AANTAL_OriginalCEN.str.contains('dozen|doos')]\n",
    "search1_df.AANTAL_OriginalCEN.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ddabb",
   "metadata": {
    "id": "524ddabb"
   },
   "source": [
    "**Your input here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf56d16",
   "metadata": {
    "id": "bcf56d16"
   },
   "outputs": [],
   "source": [
    "# Do you want to see the letters that have a specific amount in the original metadata? change the parameter inside the quotation marks for the value you are interested in. You can use regular expressions\n",
    "# for example: str.contains(r'9 dozen')\n",
    "search2 = cen_letters[cen_letters.AANTAL_OriginalCEN.str.contains(r'9 dozen')]\n",
    "search2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8bbd28",
   "metadata": {
    "id": "6c8bbd28"
   },
   "source": [
    "We also found that other information related to the type of document was added to the title metadata. We extracted that information into a column called \"quantity_DocumentTypeSk\".\n",
    "Here below it's possible to see which types of documents were described (i.e., there are not only letters in the CEN dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b51a2",
   "metadata": {
    "id": "a64b51a2"
   },
   "outputs": [],
   "source": [
    "# types of documents in the CEN dataset\n",
    "type_document_counts = letters_amount_df.quantity_DocumentTypeSk.value_counts()\n",
    "type_document_counts.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c33648",
   "metadata": {
    "id": "86c33648"
   },
   "source": [
    "**Your input here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c6bcd",
   "metadata": {
    "id": "aa5c6bcd"
   },
   "outputs": [],
   "source": [
    "# Do you want to see the letters of a specific document type? change the parameter inside the quotation marks for the value you are interested in. You can use regular expressions\n",
    "# for example: str.contains(r'Opgave|order') where the pipe symbol means \"or\"\n",
    "search3 = cen_letters[cen_letters.quantity_DocumentTypeSk.str.contains(r'Opgave|order')]\n",
    "search3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627be4db",
   "metadata": {
    "id": "627be4db"
   },
   "source": [
    "## From which years?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1735e7b2",
   "metadata": {
    "id": "1735e7b2"
   },
   "source": [
    "The original dataset has a column called \"Jaar\" (which we renamed as \"YEAR_Original\") in which cataloguers entered the date of the letter(s). However, they also added uncertainty marks in many ways (?, ca., 15XX) or ranges (1620-1630). There was also a \"Datering\" column in the original metadata. \n",
    "During the SKILLNET project we progressively cleaned this column and converted it into three columns:\n",
    "- Year: which we converted into two columns: 'yearItem_start' and 'yearItem_end'. If a letter has a single year, it is kept in the column 'yearItem_start'. If the record has a range (either for a bunch of \"letters\" with a start and and end year, or for a specific letter in which the range is used to express uncertainty), both years are registered in the two columns\n",
    "- 'monthDatingSk'\n",
    "- 'dayDatingSk'\n",
    "\n",
    "Besides, we added extra columns to keep stored the uncertainty marks, and to indicate if the year is a range, but in a standardized way. These columns have only two values: \"y\" or \"n\". We indicate here below when the value turns into \"y\":\n",
    "- 'isYearRange' : ditto\n",
    "- 'isYearRangeInferredSk': when, as part of the cleaning process, we inferred the range using, for example, the dates of birth/death of the correspondents\n",
    "- 'isYearRangeUncertain': when the range is used to express uncertainty (e.g., a letter was written between 1610 - 1630).\n",
    "- 'isYearItemInferredSk': when, as part of the research process, one of the experts found out the exact year of a letter (in that case, the range is deleted and replaced for a specific year)\n",
    "- 'isYearItemUncertain': when the specific year of a letter is inferred, thus, it's still uncertain\n",
    "- 'isDateringUncertain': when the cataloguers added any uncertainty mark, or when the datering was inferred at Skillnet but it's still uncertain\n",
    "\n",
    "Here below you can take a look to how the original metadata looked like before cleaning the \"Jaar\" and \"Datering\" columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e90f2",
   "metadata": {
    "id": "967e90f2"
   },
   "outputs": [],
   "source": [
    "# date-related columns\n",
    "letters_dates_df = cen_letters[['SKletterId','ISBD_OriginalCEN', 'TITLE_Original', 'YEAR_Original', 'yearItem_start','yearItem_end', 'isYearRange', 'yearRangeType', 'isYearRangeInferredSk', 'isYearItemInferredSk', 'isYearItemInferredSk', 'isYearItemUncertain', 'DATERING_OriginalCEN','dayDatingSk', 'monthDatingSk', 'isDateringUncertain']]\n",
    "# date-related columns to display with original data\n",
    "letters_dates_original_df = letters_dates_df[['SKletterId','YEAR_Original', 'DATERING_OriginalCEN']]\n",
    "letters_dates_original_df.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6a7e4",
   "metadata": {
    "id": "c4d6a7e4"
   },
   "source": [
    "If we would try to group the letters per year using the original \"Jaar\" column as it was, we would obtain an enormous amount of unique values, thus, if you would try to depict this in a graphic, it would be impossible to know how many letters were there per year. Here below you see how many unique values are in the original dataset in that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbb9d8",
   "metadata": {
    "id": "bdcbb9d8"
   },
   "outputs": [],
   "source": [
    "# unique values in the original CEN metadata for \"Jaar\"\n",
    "letters_dates_df.YEAR_Original.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243ef5f",
   "metadata": {
    "id": "3243ef5f"
   },
   "source": [
    "After cleaning the columns mentioned above, we got a smaller number of unique values per year only, and the months and days were separated into other two different columns. All the uncertainty marks were also added to control columns as mentioned above. Here you can take a look to the current cleaned version of the letter dates with the different columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3529d",
   "metadata": {
    "id": "04c3529d"
   },
   "outputs": [],
   "source": [
    "# date-related columns to display current cleaned data\n",
    "letters_dates_cleaned_df = letters_dates_df[['SKletterId','yearItem_start', 'yearItem_end', 'isYearRange', 'isYearRangeInferredSk', 'yearRangeType', 'isYearItemInferredSk', 'isYearItemUncertain','dayDatingSk', 'monthDatingSk', 'isDateringUncertain']]\n",
    "letters_dates_cleaned_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d35906a",
   "metadata": {
    "id": "7d35906a"
   },
   "source": [
    "The number of unique values is reduced from several thousands to a few hundreds (for the years only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ce0b7",
   "metadata": {
    "id": "f35ce0b7"
   },
   "outputs": [],
   "source": [
    "# number of unique values in the cleaned column that contains the letter year\n",
    "letters_dates_cleaned_df.yearItem_start.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b2ea0",
   "metadata": {
    "id": "524b2ea0"
   },
   "source": [
    "But not all letter dates are certain. As we mentioned, there were uncertainty marks added by the cataloguers, or some dates are not known. In some cases, we inferred the letter year(s) using an estimated range based on the dates of birth or death of the correspondents. If we want to represent the distribution of letters per year, we have to take that into account. Here below we explain in detail what the \"certain\" years and the \"uncertain\" years mean:\n",
    "\n",
    "1. certain years include:\n",
    "- a) the letters that have specific years and these years are certain.\n",
    "- b) when the CEN letter record was used to describe not only one letter but several letters. E.g., one row is to describe 60 letters, which were written between 1688 and 1680. In this case, the range doesn't indicate uncertainty, but an actual period of time in which several letters were exchanged.\n",
    "\n",
    "2. uncertain years include:  \n",
    "- c) A letter may have a specific year, but this is uncertain (as indicated by a question mark or any other uncertainty mark in the original metadata, e.g., 1520?).\n",
    "- d) It can also happen that uncertainty was indicated by adding a range (e.g., a letter was written in some year between 1688 and 1680). \n",
    "- e) Several letters described with a range (as above) may also be uncertain or incorrect. For example, in this CEN record (https://picarta.oclc.org/psi/DB=3.23/XMLPRS=Y/PPN?PPN=310885922) the range of letter exchange indicated by the cataloguer was 1641-1674 (for 3 letters). However, the dates of birth and death of the correspondents let us see that there is a mistake in that range, since one of the correspondents was already dead in the ending year of the range. For these items, we have added a \"y\" to the column \"isYearItemUncertain\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb0fe9",
   "metadata": {
    "id": "58bb0fe9"
   },
   "source": [
    "Before selecting the letters with certain years, and to make it easier to plot the distribution of letters along time, we will convert the individual years into decades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54720a",
   "metadata": {
    "id": "dd54720a"
   },
   "outputs": [],
   "source": [
    "# turning years into decades\n",
    "letters_dates_df_v2 = letters_dates_df.reset_index(drop=True).copy()\n",
    "letters_dates_df_v2['decade'] = letters_dates_df_v2['yearItem_start'] // 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9b566",
   "metadata": {
    "id": "b6c9b566"
   },
   "source": [
    "In order to represent the distribution of letters per year, we only take the subset explained under a) above. We exclude the rest for this visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19ff9b",
   "metadata": {
    "id": "1d19ff9b"
   },
   "outputs": [],
   "source": [
    "# first criterium: letter has specific years (no ranges) --> isYearRange == 'n' and the specific year of the letter is not uncertain --> isYearItemUncertain == 'n'\n",
    "# second criterium: letter has a range --> isYearRange == 'y' and this range is not uncertain --> yearRangeType == 'n'\n",
    "\n",
    "letters_dates_certain_df = letters_dates_df_v2[((letters_dates_df_v2.isYearRange == 'n') & (letters_dates_df_v2.isYearItemUncertain == 'n')) | ((letters_dates_df_v2.isYearRange == 'y') & (letters_dates_df_v2.yearRangeType == 'bunchOfLetters_certainRange'))]\n",
    "\n",
    "# display number of rows (records) which fulfill those conditions\n",
    "letters_dates_certain_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c544a0",
   "metadata": {
    "id": "c0c544a0"
   },
   "source": [
    "Now we can plot the distribution of letters with certain years (of type a: certain years for individual letters) per decade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859c856",
   "metadata": {
    "id": "0859c856"
   },
   "outputs": [],
   "source": [
    "# plot the letters per decade using only the letters that have \"certain\" years\n",
    "ax = letters_dates_certain_df.decade.value_counts().sort_index().plot(kind=\"bar\", rot=90, figsize=(20, 10))\n",
    "ax.set_title(\"Distribution of letters per decade in the CEN dataset (1200-1820), non-dated letters excluded\")\n",
    "ax.set_xlabel(\"Decades\")\n",
    "ax.set_ylabel(\"Number of letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d27def",
   "metadata": {
    "id": "09d27def"
   },
   "source": [
    "But keep in mind that many letters don't have a year. Those letters may either be excluded from the dataset, or, after the persons metadata is more complete, undergo another \"data imputation\" process to get an estimated range using the dates of birth/death of the correspondents (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec5915",
   "metadata": {
    "id": "d3ec5915"
   },
   "outputs": [],
   "source": [
    "# letters with no year\n",
    "letters_no_year = cen_letters[cen_letters.yearItem_start == 0]\n",
    "letters_no_year_count = len(letters_no_year.index)\n",
    "percentage = round((letters_no_year_count * 100 / number_rows), 2)\n",
    "print(f'Letters with no year: {letters_no_year_count}, which is {percentage}% of the total records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770231e",
   "metadata": {
    "id": "b770231e"
   },
   "outputs": [],
   "source": [
    "# see an example of those letters without year\n",
    "letters_no_year.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59134666",
   "metadata": {
    "id": "59134666"
   },
   "source": [
    "## From which archives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fb6ee",
   "metadata": {
    "id": "8d7fb6ee"
   },
   "source": [
    "The original metadata included a field called \"Signatuur\" which corresponds to the code in the filing system of the archive or library that holds the letter. Here below we can see, based on that mark, how many letters are held per each archive/library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa025640",
   "metadata": {
    "id": "fa025640"
   },
   "outputs": [],
   "source": [
    "# because some letters were merged by Skillnet when the record represented the same letter, we grouped in a column the signatuur marks. In order to get the individual values, here below we split them again\n",
    "archives_t0 = pd.DataFrame(cen_letters.Signatuur_SplitLibrarySK.str.split('|').tolist(), index=cen_letters.SKletterId).stack()\n",
    "# We now want to get rid of the secondary index\n",
    "archives_t1 = archives_t0.reset_index([0, 'SKletterId'])\n",
    "# Set the column names as we want them\n",
    "archives_t1.columns = ['SKletterId', 'Archive']\n",
    "# Group by archive name and count the number of records\n",
    "archives_count = archives_t1.groupby(['Archive'])['SKletterId'].count().sort_values(ascending=False)\n",
    "archives_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d596a0b",
   "metadata": {
    "id": "2d596a0b"
   },
   "source": [
    "# CEN Persons overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053fe5be",
   "metadata": {
    "id": "053fe5be"
   },
   "source": [
    "## Unique persons (entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8571b3",
   "metadata": {
    "id": "2e8571b3"
   },
   "source": [
    "In the original CEN dataset, the senders and receivers were entered in two different columns (afzender / ontvanger). Because these names were not entered using an authority file, and they were entered by different archives over different years, the values were highly inconsistent. There are all types of issues with these names, which we will illustrate below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15962707",
   "metadata": {
    "id": "15962707"
   },
   "source": [
    "### Original data (sender/receiver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f45040",
   "metadata": {
    "id": "60f45040"
   },
   "source": [
    "Here you can take a look into how the original data looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7826f98",
   "metadata": {
    "id": "a7826f98"
   },
   "outputs": [],
   "source": [
    "# person-related columns from letters file\n",
    "letters_persons_df = cen_letters[['SKletterId','ISBD_OriginalCEN', 'TITLE_Original', 'AFZENDER_OriginalCEN','isUncertainAfzender', 'ONTVANGER_OriginalCEN', 'isUncertainOntvanger']]\n",
    "# person-related columns to display with original data\n",
    "letters_persons_original_df = letters_persons_df[['SKletterId','ISBD_OriginalCEN', 'TITLE_Original', 'AFZENDER_OriginalCEN','ONTVANGER_OriginalCEN']]\n",
    "letters_persons_original_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec04dda",
   "metadata": {
    "id": "6ec04dda"
   },
   "source": [
    "This sample of the original data shows already some of the issues, for example:\n",
    "- Missing data: not all letters had a receiver or a sender\n",
    "- Inconsistency in the use of the CEN person Ids: some person names have an Id and some don't. \n",
    "- Some person names are properly identified with dates of birth and death, while others are not.\n",
    "- Some names have additional information (for example, a role) which is part of the name string. This makes it difficult to match the strings with other datasets where only the name is used.\n",
    "- ... etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4640320",
   "metadata": {
    "id": "d4640320"
   },
   "source": [
    "Besides inconsistency, missing data is an important issue. Mostly, organizations were not provided in the original data dump. To solve this issue, we had to extract them ourselves from the title metadata, this was mostly the case for \"ontvangers\". See for example that the receiver is mentioned in the ISBD, but it's not present in the receiver column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702a861",
   "metadata": {
    "id": "3702a861"
   },
   "outputs": [],
   "source": [
    "missing_receiver = letters_persons_original_df[letters_persons_original_df.ONTVANGER_OriginalCEN == 'null']\n",
    "missing_receiver.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19127bde",
   "metadata": {
    "id": "19127bde"
   },
   "source": [
    "But the most notable issues arise when we put the senders and the receivers together. Here you can take a look to the original data when we join all the person names in a single table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd77fea",
   "metadata": {
    "id": "0cd77fea"
   },
   "outputs": [],
   "source": [
    "# person-related columns to show original data vs cleaned data (uses AllPersons format created above to facet the column with row counts)\n",
    "cen_allPersons_list = cen_allPersons.AFZENDER_ONTVANGER_OriginalCEN.sort_values(ascending=False).value_counts()\n",
    "\n",
    "# This list display the number of ROWS in the original dataset grouped for the person (sender or receiver) as it was originally entered by the cataloguers (before any cleaning)\n",
    "cen_allPersons_list.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665d6f2",
   "metadata": {
    "id": "3665d6f2"
   },
   "source": [
    "There are many aspects to observe in the sample of the original names above, but the most important thing to notice is that most names were NN or empty ('null'), and there is a very long tale of names that only occur 2 or only once... In the original dataset there were more than 42.000 unique names, but this doesn't mean that there was such a big amount of persons or entities, they were just not consistently entered..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21fae2",
   "metadata": {
    "id": "7a21fae2"
   },
   "outputs": [],
   "source": [
    "# Here you can see how many unique values (persons/entities = strings) were in the original data\n",
    "cen_allPersons.AFZENDER_ONTVANGER_OriginalCEN.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b19f13",
   "metadata": {
    "id": "79b19f13"
   },
   "source": [
    "**Your input here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b567b",
   "metadata": {
    "id": "617b567b"
   },
   "outputs": [],
   "source": [
    "# Do you want to compare the original metadata with the cleaned version for a person? replace the name below, for example, see the variations\n",
    "# in the name of \"Adrianus Junius\": str.contains(r'.*dr.*Junius') for the name you want to consult\n",
    "\n",
    "cen_persons_original_df = cen_persons[['SKpersonId','CENPersonIds','AFZENDER_ONTVANGER_OriginalCEN', 'personStrId']]\n",
    "search3 = cen_persons_original_df[cen_persons_original_df.AFZENDER_ONTVANGER_OriginalCEN.str.contains(r'.*dr.*Junius')]\n",
    "search3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16613059",
   "metadata": {
    "id": "16613059"
   },
   "source": [
    "As you see in the first line above, in the column \"Afzender_Ontvanger_OriginalCEN\", separated by a pipe symbol (|) there are the different variants of the name in the original metadata. Some of them had a CEN person Id but some don't, some have person-related dates, but others don't. Most of the normalization process results in clustering different variant forms of a name string into a single and indentified person. Of course there may be also mistakes in the clustering, which was done semi-automatically, but there was also a great deal of manual research involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12743d4",
   "metadata": {
    "id": "c12743d4"
   },
   "source": [
    "### Types of unique entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb546c",
   "metadata": {
    "id": "e1bb546c"
   },
   "source": [
    "But in the CEN dataset there were not only persons involved as correspondents, there are also other entities such as organizations or groups. And, in some cases, the names are generic (for example: \"zijn zus\". We classified the types of entities. Here below you can see the types and their amount. The cleaning process up to date (July 7, 2022) has mostly been focused on the \"namedPersons\". Future work includes the cleaning up of the organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec8013",
   "metadata": {
    "id": "3dec8013"
   },
   "outputs": [],
   "source": [
    "# types of \"entities\"\n",
    "allPersons_letters.nameType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5b221",
   "metadata": {
    "id": "06b5b221"
   },
   "source": [
    "## Letters per person/entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95beb76d",
   "metadata": {
    "id": "95beb76d"
   },
   "source": [
    "In this section we will visualize the distribution of letters per person or entity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b64e12",
   "metadata": {
    "id": "80b64e12"
   },
   "source": [
    "### Letters per unique senders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce4b64",
   "metadata": {
    "id": "46ce4b64"
   },
   "source": [
    "Here below you can see a list of the senders ordered per the amount of letters they sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce64a1",
   "metadata": {
    "id": "73ce64a1"
   },
   "outputs": [],
   "source": [
    "senders_letters_sum_df = cen_allPersons[cen_allPersons.SKletterIdWithRole.str.contains('_afzender')]\n",
    "senders_letters_sum = senders_letters_sum_df.groupby(['personStrId'])['quantity_CleanedSk'].sum().reset_index()\n",
    "# sort values\n",
    "sorted_senders_letters_sum = senders_letters_sum.sort_values(ascending=False,by='quantity_CleanedSk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6982b8a",
   "metadata": {
    "id": "e6982b8a"
   },
   "source": [
    "We set it up to display a list with 500 names... If you would like to display a different number, change the value inside the parenthesis after head. If you want to display the entire list, delete this part: .head(500). If you want to display the entire list (be patient!) it should look like this: \n",
    "\n",
    "sorted_senders_letters_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af4684",
   "metadata": {
    "id": "45af4684"
   },
   "outputs": [],
   "source": [
    "# reset the index to avoid confusion with index number\n",
    "sorted_senders_letters_sum_final = sorted_senders_letters_sum.reset_index(drop=True)\n",
    "\n",
    "# display the list of senders in descending order of number of letters sent\n",
    "sorted_senders_letters_sum_final.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7010a80",
   "metadata": {
    "id": "c7010a80"
   },
   "outputs": [],
   "source": [
    "# number of senders\n",
    "sorted_senders_letters_sum.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa46c4",
   "metadata": {
    "id": "dbaa46c4"
   },
   "source": [
    "**Your input here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b30c71",
   "metadata": {
    "id": "15b30c71"
   },
   "source": [
    "Because we cannot plot all the list with all senders, we have to make a smaller selection. Here below the code is set to show the top 20. You can change that value if you want to show less or more senders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924fb615",
   "metadata": {
    "id": "924fb615"
   },
   "outputs": [],
   "source": [
    "# determine how many persons should be shown\n",
    "top_s = 20\n",
    "# create small df for displaying and plotting\n",
    "letters_per_sender = sorted_senders_letters_sum.head(top_s).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493cebf4",
   "metadata": {
    "id": "493cebf4"
   },
   "outputs": [],
   "source": [
    "# plotting in a barh chart the top n senders\n",
    "ax = senders_letters_sum_df.groupby(['personStrId'])['quantity_CleanedSk'].sum().sort_values(ascending=True).tail(top_s).plot(kind='barh', figsize=(20, 10))\n",
    "ax.set_title(\"Top senders in the CEN dataset (1200-1820)\")\n",
    "ax.set_xlabel(\"Number of letters\")\n",
    "ax.set_ylabel(\"Sender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d278d13",
   "metadata": {
    "id": "4d278d13"
   },
   "source": [
    "### Letters per unique receivers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a09bbe",
   "metadata": {
    "id": "29a09bbe"
   },
   "source": [
    "Here below you can see a list of the receivers ordered per the amount of letters they received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a48b2b",
   "metadata": {
    "id": "07a48b2b"
   },
   "outputs": [],
   "source": [
    "receivers_letters_sum_df = cen_allPersons[cen_allPersons.SKletterIdWithRole.str.contains('_ontvanger')]\n",
    "receivers_letters_sum = receivers_letters_sum_df.groupby(['personStrId'])['quantity_CleanedSk'].sum().reset_index()\n",
    "# sort values\n",
    "sorted_receivers_letters_sum = receivers_letters_sum.sort_values(ascending=False,by='quantity_CleanedSk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7a94db",
   "metadata": {
    "id": "de7a94db"
   },
   "source": [
    "We set it up to display a list with 500 names... If you would like to display a different number, change the value inside the parenthesis after head. If you want to display the entire list, delete this part: .head(500). If you want to display the entire list (be patient!) it should look like this: \n",
    "\n",
    "sorted_receivers_letters_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b92526",
   "metadata": {
    "id": "12b92526"
   },
   "outputs": [],
   "source": [
    "sorted_receivers_letters_sum.head(100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75518103",
   "metadata": {
    "id": "75518103"
   },
   "outputs": [],
   "source": [
    "# number of receivers\n",
    "sorted_receivers_letters_sum.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47aff3d",
   "metadata": {
    "id": "c47aff3d"
   },
   "source": [
    "**Your input here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b6892",
   "metadata": {
    "id": "591b6892"
   },
   "source": [
    "Because we cannot plot all the list with all senders, we have to make a smaller selection. Here below the code is set to show the top 20. You can change that value if you want to show less or more senders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a1fca",
   "metadata": {
    "id": "8c7a1fca"
   },
   "outputs": [],
   "source": [
    "# determine how many persons should be shown\n",
    "top_r = 20\n",
    "# create small df for displaying and plotting\n",
    "letters_per_receiver = sorted_receivers_letters_sum.head(top_r).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708d23b",
   "metadata": {
    "id": "7708d23b"
   },
   "outputs": [],
   "source": [
    "# plotting in a barh chart the top n receivers\n",
    "ax = receivers_letters_sum_df.groupby(['personStrId'])['quantity_CleanedSk'].sum().sort_values(ascending=True).tail(top_r).plot(kind='barh', figsize=(20, 10))\n",
    "ax.set_title(\"Top receivers in the CEN dataset (1200-1820)\")\n",
    "ax.set_xlabel(\"Number of letters\")\n",
    "ax.set_ylabel(\"Receiver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63111e29",
   "metadata": {
    "id": "63111e29"
   },
   "source": [
    "### Letters per unique entity (person, organization, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bb445",
   "metadata": {
    "id": "836bb445"
   },
   "source": [
    "Here below you can see a list of the entities (persons, organizations, etc.) ordered per the amount of letters they appear in as correspondents (sender or receiver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b4506",
   "metadata": {
    "id": "741b4506"
   },
   "outputs": [],
   "source": [
    "person_letters_sum_df = allPersons_letters.copy()\n",
    "person_letters_sum = person_letters_sum_df.groupby(['personStrId'])['quantity_CleanedSk'].sum().reset_index()\n",
    "# sort values\n",
    "sorted_person_letters_sum = person_letters_sum.sort_values(ascending=False,by='quantity_CleanedSk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d579b0",
   "metadata": {
    "id": "83d579b0"
   },
   "source": [
    "We set it up to display a list with 500 names... If you would like to display a different number, change the value inside the parenthesis after head. If you want to display the entire list, delete this part: .head(500). If you want to display the entire list (be patient!) it should look like this: \n",
    "\n",
    "sorted_person_letters_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c056f7e",
   "metadata": {
    "id": "7c056f7e"
   },
   "outputs": [],
   "source": [
    "sorted_person_letters_sum.head(100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d988bf2",
   "metadata": {
    "id": "8d988bf2"
   },
   "source": [
    "**Your input here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c679be",
   "metadata": {
    "id": "e6c679be"
   },
   "source": [
    "Because we cannot plot all the list with all senders, we have to make a smaller selection. Here below the code is set to show the top 20. You can change that value if you want to show less or more senders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb5aa1",
   "metadata": {
    "id": "4ccb5aa1"
   },
   "outputs": [],
   "source": [
    "# determine how many persons should be shown\n",
    "top_p = 20\n",
    "# create small df for displaying and plotting\n",
    "letters_per_person = sorted_person_letters_sum.head(top_p).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e70408",
   "metadata": {
    "id": "77e70408"
   },
   "outputs": [],
   "source": [
    "# plotting in a barh chart the top n correspondents\n",
    "ax = person_letters_plot = person_letters_sum_df.groupby(['personStrId'])['quantity_CleanedSk'].sum().sort_values(ascending=True).tail(top_p).plot(kind='barh', figsize=(20, 10))\n",
    "ax.set_title(\"Top correspondents in the CEN dataset (1200-1820)\")\n",
    "ax.set_xlabel(\"Number of letters\")\n",
    "ax.set_ylabel(\"Correspondent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b5799",
   "metadata": {
    "id": "eb6b5799"
   },
   "source": [
    "## Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa164e14",
   "metadata": {
    "id": "aa164e14"
   },
   "source": [
    "Because of the size of the CEN dataset, and due to the fact that it includes letters from different archives and persons, the opportunities for network analysis that it offers are unique. In this section we will explore the first-degree network of a person of interest (for you to select) and also visualize the second-degree network of that person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f1bd6",
   "metadata": {
    "id": "690f1bd6"
   },
   "source": [
    "Based on the letters file, it's possible to generate a list of the persons a correspondent kept direct contact with (the first-degree network). The format that we have generated has the list per each person in the dataset. However, it would be easier to see the network of a person you are interested in. Thus, this is done in three steps:\n",
    "- First: consult which is the Id of the person you are interested by quering per name\n",
    "- Second: enter the Id that you found in the code that generates the list\n",
    "- Third: generate the visualization (this notebook only goes to the step of generating the \"nodes\" and \"edges\" files, which can be imported to Gephi or other network tools/libraries to be visualized or further processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ff5fc",
   "metadata": {},
   "source": [
    "### Choose your person of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c4a1f",
   "metadata": {
    "id": "bb2c4a1f"
   },
   "outputs": [],
   "source": [
    "# create a small df to query for the Id of a person using it's name or date of birth, death or floriat\n",
    "basicInfo_person_df = cen_persons[['SKpersonId', 'personStrId', 'dateBirth', 'dateDeath', 'dateFl']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a369d9",
   "metadata": {
    "id": "58a369d9"
   },
   "source": [
    "**Your input!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f14a5a",
   "metadata": {
    "id": "15f14a5a"
   },
   "source": [
    "The purpose of the cell below is to query the list of persons in the dataset and obtain his/her Id. The Id is used to query the letter dataset to find the first-degree contacts.\n",
    "You can also use regular expressions to query for a name (that is the meaning of the letter \"r\" inside the parenthesis. For example: \n",
    "- because 'Leeuwenhoek' may be written in different ways, you could use: (r'Ant.*Lee.*') --> this means that the name starts with Ant and the last name starts with Lee, but you are not sure of what the words after are. Please notice that it is case sensitive\n",
    "- if you hesitate between two variants, for example Adriaan and Adrianus, and you know the last name, you can use or (the pipe symbol): (r'Adriaan.*Junius|Adrianus.*Junius'). Remember that a dot represents any character, and an asteriks represents 1 or more times. Then, we are looking for any characters between the name and the last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26ead0",
   "metadata": {
    "id": "3e26ead0"
   },
   "outputs": [],
   "source": [
    "# First step: query using the name of the person you are interested in and copy the SKpersonId\n",
    "person_of_interest = basicInfo_person_df[basicInfo_person_df.personStrId.str.contains(r'Huygens')] # I selected here a person with a small network for display purposes\n",
    "person_of_interest.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34934b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second step: paste the person Id below (inside the quotation marks)\n",
    "person_of_interest = 'skp007667'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bd78c",
   "metadata": {
    "id": "fe0bd78c"
   },
   "source": [
    "### First-degree network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8582726",
   "metadata": {
    "id": "f8582726"
   },
   "outputs": [],
   "source": [
    "# here we create a dataframe with the letters written or sent by the person of interest (the first-degree then)\n",
    "firstDegree_letters_df = cen_letters[(cen_letters.SKpersonId_sender.str.contains(person_of_interest)) | (cen_letters.SKpersonId_receiver.str.contains(person_of_interest))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266032ff",
   "metadata": {
    "id": "266032ff"
   },
   "outputs": [],
   "source": [
    "# Display the letters in the first-degree network of a person of interest \n",
    "#(remember to change the number within head(#) if you want to see more)\n",
    "# If you want to download this file, section 6 offers the Export options.\n",
    "\n",
    "firstDegree_letters_df.head(5).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306e8f2",
   "metadata": {},
   "source": [
    "You may want to see how many letters are in the first-degree network dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOW MANY LETTER RECORDS ARE IN THE FIRST DEGREE? (NUMBER OF RECORDS, doesn't count the letters but the rows)\n",
    "firstDegree_letters_df.shape[0]\n",
    "\n",
    "#!! WARNING: these are the number of records in the dataset, remember that records are not equivalent to number of letters (see section 4.2: How many letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58705b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY LETTERS ARE IN THE FIRST-DEGREE NETWORK? (NUMBER OF LETTERS, it sums the number of letters per record)\n",
    "firstDegree_sum_df = firstDegree_letters_df.quantity_CleanedSk.sum()\n",
    "firstDegree_sum_df\n",
    "\n",
    "# !!WARNING: remember that in some cases one row (i.e., one record) may have a zero which represents \n",
    "# an unknown amount of letters (see section 4.2. How many letters above)\n",
    "# So, this total is of the number of letters for a person of interest of which the exact\n",
    "# number of letters in a record is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca382b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records have a zero as number of letters?\n",
    "# Here we can see how many records of the type \"brieven\" had an \"undetermined\" value in the subset of the first-degree correspondence for a person of interest\n",
    "brieven_person_interest_undet_firstD = firstDegree_letters_df[(firstDegree_letters_df.quantity_CleanedSk == 0)]\n",
    "len(brieven_person_interest_undet_firstD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb4e87",
   "metadata": {},
   "source": [
    "You may want to see how many correspondents (people) are in the first-degree network dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2972f273",
   "metadata": {
    "id": "2972f273"
   },
   "outputs": [],
   "source": [
    "# generate df with unique first-degree corespondents (SKpersonId + personStrId)\n",
    "correspondents_df1 = firstDegree_letters_df[['SKpersonId_sender','personStrId_sender', 'SKpersonId_receiver', 'personStrId_receiver']]\n",
    "firstDegree_senders = correspondents_df1[['SKpersonId_sender','personStrId_sender']].copy()\n",
    "firstDegree_senders.rename(columns={'SKpersonId_sender':'SKpersonId', 'personStrId_sender':'personStrId'},inplace=True)\n",
    "firstDegree_receivers = correspondents_df1[['SKpersonId_receiver','personStrId_receiver']].copy()\n",
    "firstDegree_receivers.rename(columns={'SKpersonId_receiver':'SKpersonId', 'personStrId_receiver':'personStrId'},inplace=True)\n",
    "firstDegree_all = pd.concat([firstDegree_senders,firstDegree_receivers])\n",
    "firstDegree_unique = firstDegree_all.drop_duplicates()\n",
    "firstDegree_persons = firstDegree_unique.reset_index(drop=True)\n",
    "firstDegree_persons.sort_values(by='personStrId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972676e",
   "metadata": {
    "id": "4972676e"
   },
   "outputs": [],
   "source": [
    "# HOW BIG IS THE FIRST-DEGREE NETWORK? (NUMBER OF PEOPLE)\n",
    "firstDegree_persons.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce427f",
   "metadata": {
    "id": "52ce427f"
   },
   "source": [
    "### Second-degree network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fdc1c",
   "metadata": {
    "id": "598fdc1c"
   },
   "source": [
    "Visualize a first-degree network is not very interesting. Thus, we will capture now the second-degree network and represent it visually. A second degree network consist of the first-degree correspondents of each person above. We will then combine the first and second-degree network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d1a73",
   "metadata": {
    "id": "d97d1a73"
   },
   "outputs": [],
   "source": [
    "# make a copy of the letters df\n",
    "secondDegree_letters_df_t01 = cen_letters.copy()\n",
    "# do an inner merge of the senders with the list of first-degree correspondents\n",
    "letters_2ndD_senders_df = secondDegree_letters_df_t01.merge(firstDegree_persons, left_on='SKpersonId_sender', right_on='SKpersonId', how='inner')\n",
    "# do an inner merge of the receivers with the list of first-degree correspondents\n",
    "letters_2ndD_receivers_df = secondDegree_letters_df_t01.merge(firstDegree_persons, left_on='SKpersonId_receiver', right_on='SKpersonId', how='inner')\n",
    "# to get the letters df I need to append and drop duplicate rows\n",
    "secondDegree_letters_df_all = pd.concat([letters_2ndD_senders_df,letters_2ndD_receivers_df])\n",
    "secondDegree_letters_df = secondDegree_letters_df_all.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the letters in the second-degree network of a person of interest \n",
    "#(remember to change the number within head(#) if you want to see more)\n",
    "# If you want to download this file, section 6 offers the Export options.\n",
    "\n",
    "secondDegree_letters_df.head(5).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11c15a",
   "metadata": {
    "id": "8f11c15a"
   },
   "source": [
    "You may want to see how many letters are in the second-degree network dataset, here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454942a4",
   "metadata": {
    "id": "454942a4"
   },
   "outputs": [],
   "source": [
    "#HOW MANY LETTER RECORDS ARE IN THE SECOND DEGREE? (NUMBER OF RECORDS, doesn't count the letters but the rows)\n",
    "secondDegree_letters_df.shape[0]\n",
    "\n",
    "#!! WARNING: these are the number of records in the dataset, remember that records are not equivalent to number of letters (see section 4.2: How many letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY LETTERS ARE IN THE SECOND-DEGREE NETWORK? (NUMBER OF LETTERS, it sums the number of letters per record)\n",
    "secondDegree_sum_df = secondDegree_letters_df.quantity_CleanedSk.sum()\n",
    "secondDegree_sum_df\n",
    "\n",
    "# !!WARNING: remember that in some cases one row (i.e., one record) may have a zero which represents \n",
    "# an unknown amount of letters (see section 4.2. How many letters above)\n",
    "# So, this total is of the number of letters for a person of interest of which the exact\n",
    "# number of letters in a record is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records have a zero as number of letters?\n",
    "# Here we can see how many records of the type \"brieven\" had an \"undetermined\" value in the subset of the second-degree correspondence for a person of interest\n",
    "brieven_person_interest_undet_2ndD = secondDegree_letters_df[(secondDegree_letters_df.quantity_CleanedSk == 0)]\n",
    "len(brieven_person_interest_undet_2ndD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa7b7a9",
   "metadata": {},
   "source": [
    "You may want to see how many correspondents (people) are in the first-degree network dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832fdc1",
   "metadata": {
    "id": "2832fdc1"
   },
   "outputs": [],
   "source": [
    "# generate df with unique second-degree corespondents (SKpersonId + personStrId)\n",
    "correspondents_df2 = secondDegree_letters_df[['SKpersonId_sender','personStrId_sender', 'SKpersonId_receiver', 'personStrId_receiver']]\n",
    "secondDegree_senders = correspondents_df2[['SKpersonId_sender','personStrId_sender']].copy()\n",
    "secondDegree_senders.rename(columns={'SKpersonId_sender':'SKpersonId', 'personStrId_sender':'personStrId'},inplace=True)\n",
    "secondDegree_receivers = correspondents_df2[['SKpersonId_receiver','personStrId_receiver']].copy()\n",
    "secondDegree_receivers.rename(columns={'SKpersonId_receiver':'SKpersonId', 'personStrId_receiver':'personStrId'},inplace=True)\n",
    "secondDegree_persons_all = pd.concat([secondDegree_senders,secondDegree_receivers])\n",
    "secondDegree_persons_unique = secondDegree_persons_all.drop_duplicates()\n",
    "secondDegree_persons = secondDegree_persons_unique\n",
    "secondDegree_persons.sort_values(by='personStrId').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e08b1",
   "metadata": {
    "id": "5b9e08b1"
   },
   "outputs": [],
   "source": [
    "# display how big is the secon-degree network (NUMBER OF 2nd-DEGREE CONTACTS)\n",
    "secondDegree_persons.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90619ad4",
   "metadata": {
    "id": "90619ad4"
   },
   "source": [
    "### Generate network data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06ea96",
   "metadata": {
    "id": "bc06ea96"
   },
   "source": [
    "Even though the python network visualizations and analysis libraries are very powerful (here not yet used to their maximum possibilities), one may be more familiar with other tools, for example Gephi. Most network visualization/analysis tools need a data input in the format: Source/Target/Weight.\n",
    "Here below we provide the first two columns, and the nodes file, ready to import to Gephi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271ebc1",
   "metadata": {},
   "source": [
    "#### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacaef4b",
   "metadata": {
    "id": "cacaef4b"
   },
   "outputs": [],
   "source": [
    "# First prepare the nodes (i.e., the persons). Gephi requires these labeled as Id,Name\n",
    "nodes_t01 = secondDegree_persons.reset_index(drop=True).copy()\n",
    "nodes_t01.rename(columns={'SKpersonId':'Id', 'personStrId':'Name'},inplace=True)\n",
    "nodes_t02 = nodes_t01.copy()\n",
    "# replacing commas in the name for semicoloms, because the comma will be used as separator\n",
    "nodes_t02['Name'].replace(to_replace=r',', value=';', regex=True).copy()\n",
    "nodes = nodes_t02.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ef97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57f908",
   "metadata": {},
   "source": [
    "#### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01ce80",
   "metadata": {
    "id": "8f01ce80"
   },
   "outputs": [],
   "source": [
    "# Then prepare the edges (i.e., the relations). Gephi requires these labeled as Source,Target\n",
    "# first in order sender-receiver\n",
    "edges_t01 = secondDegree_letters_df.reset_index(drop=True).copy()\n",
    "edges_t02 = edges_t01[['SKpersonId_sender', 'SKpersonId_receiver']].copy()\n",
    "edges_t02.rename(columns={'SKpersonId_sender':'Source', 'SKpersonId_receiver':'Target'},inplace=True)\n",
    "edges_t03 = edges_t02.drop_duplicates()\n",
    "edges = edges_t03.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f71c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8115a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae02d85",
   "metadata": {
    "id": "0ae02d85"
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307a1c3",
   "metadata": {},
   "source": [
    "### Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of (identified) women\n",
    "women_unique = cen_persons.sex.value_counts()\n",
    "women_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cb06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of women (met letter counts)\n",
    "women_letters_sum_df = allPersons_letters[allPersons_letters.sex.str.contains('f')]\n",
    "women_letters_sum = women_letters_sum_df.groupby(['personStrId'])['quantity_CleanedSk'].sum().reset_index()\n",
    "# sort values\n",
    "sorted_women_letters_sum = women_letters_sum.sort_values(ascending=False,by='quantity_CleanedSk')\n",
    "sorted_women_letters_sum.head(2000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0c5d3",
   "metadata": {},
   "source": [
    "### Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b094df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common roles in the dataset. These were not included as a separated metadata in the original dataset, \n",
    "# but we extracted them from the titles, original names, and other descriptions\n",
    "# these values are still messy (cleaning in progress)\n",
    "roles_unique = cen_persons.rolesOccupationsTitles.value_counts()\n",
    "roles_unique.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many kings do you think are in the dataset?\n",
    "\n",
    "search_role_df = cen_persons[['SKpersonId', 'personStrId', 'rolesOccupationsTitles']]\n",
    "search_role = search_role_df[search_role_df.rolesOccupationsTitles.str.contains(r'.oning$')].reset_index(drop=True)\n",
    "search_role.SKpersonId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1764af",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e460a6",
   "metadata": {},
   "source": [
    "### Ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc7b69",
   "metadata": {
    "id": "42bc7b69"
   },
   "outputs": [],
   "source": [
    "# earliest correspondents\n",
    "dateBirth_nonZero_df = basicInfo_person_df[basicInfo_person_df['dateBirth'] != 0]\n",
    "earliest_person_df = dateBirth_nonZero_df.sort_values(by='dateBirth', ascending=True)\n",
    "earliest_person_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700dba4",
   "metadata": {
    "id": "d700dba4"
   },
   "outputs": [],
   "source": [
    "# latest correspondents\n",
    "earliest_person_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5f687",
   "metadata": {
    "id": "b45f8df7"
   },
   "source": [
    "# Exports\n",
    "\n",
    "The files exported here will be downloaded to the data/processed directory (see paths at the beginning of the notebook). \n",
    "If you are using MyBinder locate the files on the left panel (icon with a directory folder, navigate to the data/processed folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45dda2",
   "metadata": {},
   "source": [
    "## All letters\n",
    "Even though you could download the entire cleaned CEN dataset from this notebook, we recommend to use the latest version deposited in Dataverse:\n",
    "The complete description and the dataset is available in Dataverse (https://dataverse.nl/dataverse/skillnet). The specific dataset that is used by this notebook can be cited with this DOI (add always version number to the citation): https://doi.org/10.34894/G8XQI0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32628782",
   "metadata": {
    "id": "32628782"
   },
   "source": [
    "## Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32916b05",
   "metadata": {
    "id": "32916b05"
   },
   "outputs": [],
   "source": [
    "# Here below are the commands to export the two files (nodes and edges) generated in section 5.3.\n",
    "# These files are ready to upload to Gephi. \n",
    "\n",
    "# this line will add the time stamp to the file name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# store nodes file\n",
    "nodes_file = (f\"nodes_{timestr}.csv\")\n",
    "nodes_path = os.path.join(data_processed_directory, nodes_file)\n",
    "nodes.to_csv(nodes_path, index = False)\n",
    "\n",
    "# store edges file\n",
    "edges_file = (f\"edges_{timestr}.csv\")\n",
    "edges_path = os.path.join(data_processed_directory, edges_file)\n",
    "edges.to_csv(edges_path, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c5a50",
   "metadata": {},
   "source": [
    "## Letters by/to person of interest (first degree)\n",
    "In section 5.3 you chose a person of interest. Here you can download the slice (i.e., portion of the CEN dataset) of the first-degree correspondence of this person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line will add the time stamp to the file name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# store first-degree letters file\n",
    "letters_1stDegree_file = (f\"letters_{person_of_interest}_1stDegree_{timestr}.csv\")\n",
    "letters_1stDegree_path = os.path.join(data_processed_directory, letters_1stDegree_file)\n",
    "firstDegree_letters_df.to_csv(letters_1stDegree_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ded6ee",
   "metadata": {},
   "source": [
    "## Letters by/to person of interest (second degree)\n",
    "In section 5.3 you chose a person of interest. Here you can download the slice (i.e., portion of the CEN dataset) of the second-degree correspondence of this person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line will add the time stamp to the file name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# store second-degree letters file\n",
    "letters_2ndDegree_file = (f\"letters_{person_of_interest}_2ndDegree_{timestr}.csv\")\n",
    "letters_2ndDegree_path = os.path.join(data_processed_directory, letters_2ndDegree_file)\n",
    "secondDegree_letters_df.to_csv(letters_2ndDegree_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "509px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
