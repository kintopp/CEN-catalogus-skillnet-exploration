{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5b400cd4",
      "metadata": {
        "toc": true,
        "id": "5b400cd4"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Background</a></span></li><li><span><a href=\"#How-to-use-this-notebook\" data-toc-modified-id=\"How-to-use-this-notebook-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>How to use this notebook</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Tests</a></span></li></ul></li><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import libraries</a></span></li><li><span><a href=\"#Import-&amp;-prepare-data\" data-toc-modified-id=\"Import-&amp;-prepare-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Import &amp; prepare data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Letters-file\" data-toc-modified-id=\"Letters-file-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Letters file</a></span></li><li><span><a href=\"#UniquePersons-file\" data-toc-modified-id=\"UniquePersons-file-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>UniquePersons file</a></span></li><li><span><a href=\"#AllPersons-format-(processing)\" data-toc-modified-id=\"AllPersons-format-(processing)-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>AllPersons format (processing)</a></span></li><li><span><a href=\"#AllPersons-format-(combined-with-uniquePersons)-(processing)\" data-toc-modified-id=\"AllPersons-format-(combined-with-uniquePersons)-(processing)-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>AllPersons format (combined with uniquePersons) (processing)</a></span></li></ul></li><li><span><a href=\"#CEN-Letters-overview\" data-toc-modified-id=\"CEN-Letters-overview-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>CEN Letters overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#CEN-dataset-description\" data-toc-modified-id=\"CEN-dataset-description-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>CEN dataset description</a></span></li><li><span><a href=\"#How-many-letters?\" data-toc-modified-id=\"How-many-letters?-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>How many letters?</a></span></li><li><span><a href=\"#From-which-years?\" data-toc-modified-id=\"From-which-years?-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>From which years?</a></span></li><li><span><a href=\"#From-which-archives?\" data-toc-modified-id=\"From-which-archives?-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>From which archives?</a></span></li></ul></li><li><span><a href=\"#CEN-Persons-overview\" data-toc-modified-id=\"CEN-Persons-overview-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CEN Persons overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unique-persons-(entities)\" data-toc-modified-id=\"Unique-persons-(entities)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Unique persons (entities)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Original-data-(sender/receiver)\" data-toc-modified-id=\"Original-data-(sender/receiver)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Original data (sender/receiver)</a></span></li><li><span><a href=\"#Types-of-unique-entities\" data-toc-modified-id=\"Types-of-unique-entities-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Types of unique entities</a></span></li></ul></li><li><span><a href=\"#Letters-per-person/entity\" data-toc-modified-id=\"Letters-per-person/entity-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Letters per person/entity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Letters-per-unique-senders\" data-toc-modified-id=\"Letters-per-unique-senders-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Letters per unique senders</a></span></li><li><span><a href=\"#Letters-per-unique-receivers\" data-toc-modified-id=\"Letters-per-unique-receivers-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Letters per unique receivers</a></span></li><li><span><a href=\"#Letters-per-unique-entity-(person,-organization,-etc.)\" data-toc-modified-id=\"Letters-per-unique-entity-(person,-organization,-etc.)-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Letters per unique entity (person, organization, etc.)</a></span></li></ul></li><li><span><a href=\"#Relations\" data-toc-modified-id=\"Relations-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Relations</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-degree-network\" data-toc-modified-id=\"First-degree-network-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>First-degree network</a></span></li><li><span><a href=\"#Second-degree-network\" data-toc-modified-id=\"Second-degree-network-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Second-degree network</a></span></li><li><span><a href=\"#Export-relations-data\" data-toc-modified-id=\"Export-relations-data-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Export relations data</a></span></li></ul></li><li><span><a href=\"#Curiosa\" data-toc-modified-id=\"Curiosa-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Curiosa</a></span></li></ul></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94849123",
      "metadata": {
        "id": "94849123"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f293a280",
      "metadata": {
        "id": "f293a280"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb61de0",
      "metadata": {
        "id": "1fb61de0"
      },
      "source": [
        "This jupyter notebook has been created in the context of the SKILLNET project (skillnet.nl/). The SKILLNET project (\"Sharing Knowledge in Learned and Literary Networks\") is an European Research Council (ERC)-funded project led by Dirk van Miert (1) at Utrecht University.\n",
        "\n",
        "One of the most important datasets curated during this project is the Catalogus Epistolarum Neerlandicarum (CEN). This is an aggregated letter dataset of letters' metadata created at different archives and libraries in The Netherlands since the years 1980's which contains more than five hundred thousand records. Since January 2020, one can consult the CEN via Worldcat (https://picarta.on.worldcat.org (last accessed December 29, 2021). The CEN dataset was obtained by Ingeborg van Vugt (2) as a dump in XML format from the Online Computer Library Center (OCLC) and the Royal Dutch Library (KB) in October 2019. The dataset has been sliced (years between 1200 to 1820) and cleaned during the SKILLNET project. The cleaning process of that data dump has been carried out by Liliana Melgar Estrada (3) and Ingeborg van Vugt, receiving the input from different collaborators (4). The complete description and the data itself are available in Dataverse (5).\n",
        "\n",
        "This notebook provides access to the CEN catalog slice cleaned during the project, and offers basic functionalities (using the Python library \"Pandas\" and other libraries for visualization and network analysis) to query and visualize the data. \n",
        "\n",
        "This data overview is offered to researchers as a basic way to see \"what's in the data\", but for more complex analyses it is recommended to download the latest version of the entire dataset from Dataverse (5).\n",
        "\n",
        "---\n",
        "- (1) Dirk van Miert: SKILLNET project leader. https://orcid.org/0000-0002-5460-4075\n",
        "- (2) Ingeborg van Vugt: postdoctoral researcher at SKILLNET, data curator of the CEN dataset. https://orcid.org/0000-0002-7703-1791\n",
        "- (3) Liliana Melgar Estrada: data manager at SKILLNET, data curator of the CEN dataset and author of this jupyter notebook. https://orcid.org/0000-0003-2003-4200\n",
        "- (4) The SKILLNET project thanks the students and collaborators who gave input to curate this data. They are all listed in the Dataverse page for the dataset.\n",
        "- (5) All datasets offered by the SKILLNET project are available in this Dataverse: https://dataverse.nl/dataverse/skillnet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccee7e7f",
      "metadata": {
        "id": "ccee7e7f"
      },
      "source": [
        "## How to use this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f5eab3",
      "metadata": {
        "id": "67f5eab3"
      },
      "source": [
        "This is a jupyter notebook (6). Jupyter notebooks are web environments where code can be written and executed, and it is not only code, but it also allows the integration of narrative text. Jupyter notebooks can be ran in your own computer by installing them. In this case, we offer the notebook in a cloud service provided by Google, so you don't need to install anything. The service is called \"Collab\" (7).\n",
        "\n",
        "The notebook has cells with python code that can be executed, and other cells with explanatory text written in Markdown (which is a basic text editing language) (8). Below a cell with code there is often a cell with output. \n",
        "\n",
        "This notebook is meant to work without having to write any code yourself in order to get those outputs. If you click in the main menu above the option \"Cell -> Run all\", the notebook will produce results and visualizations, which are explained in the explanatory text cells. However, there is also the option to do simple manipulations: for example, instead of getting the first 20 correspondents, you can decide you want to see 50. In that case, you need to change that value in the cell. When this is the case, the cell has a header above named \"Your input here\".\n",
        "\n",
        "You can also add comments to this notebook (for example: suggestions to add new functions, questions, requests for more explanation, etc.). For doing this, you can use the \"add comment\" functionality in the cell pannel. Another option is to click on \"Insert\" in the main menu above, and insert a new cell. Change it's type to \"Markdown\" in the menu below the main menu. Then you can write in simple text all your comments.\n",
        "\n",
        "If you want to read the notebook step by step, you can then:\n",
        "- One option is that you run one cell at the time. However, Section 2 and 3 are mostly preparatory (you don't get any interesting output from them), thus, you can skip those, but you will have to execute them. For that purpose put your cursor in Section 4: CEN Letters overview. Click on the menu \"Cell -> Run all above\". This is to execute all the cells that download and prepare the data.\n",
        "- Then you can go cell by cell reading the explanations and executing the code to get the outputs.\n",
        "- When you see a header \"Give input here\" you can then change the values according to your wishes.\n",
        "- Some cells are meant to facilitate exporting the data. Those cells are \"commented\". This means that they are not executed unless you decide so. Otherwise, every time that you run the notebook you will get many files downloaded to your computer without you having decided that. The symbol to distinguish a comment is the dash: #. If a cell is commented, all the code inside it has a dash or two dashes. In order to uncomment it, you can select all the content of the cell and press \"CTRL\" + /.\n",
        "\n",
        "---\n",
        "- (6) https://jupyter.org/\n",
        "- (7) https://colab.research.google.com/notebooks/basic_features_overview.ipynb\n",
        "- (8) https://en.wikipedia.org/wiki/Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09d8f7a",
      "metadata": {
        "id": "f09d8f7a"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188878cb",
      "metadata": {
        "id": "188878cb"
      },
      "source": [
        "- Test1: This is a text cell (see the type above: \"Markdown\").\n",
        "- Click on the button \"Run\" above and see what happens (nothing should happen). \n",
        "- Now change the type of cell to \"Code\" and run it again (now you should get an error)... change the type back to \"Markdown\".\n",
        "\n",
        "This is hard to read"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I can write anything"
      ],
      "metadata": {
        "id": "SKfvrPOr8cE_"
      },
      "id": "SKfvrPOr8cE_"
    },
    {
      "cell_type": "code",
      "source": [
        "print('hello')"
      ],
      "metadata": {
        "id": "RcVGPeVH8eN7"
      },
      "id": "RcVGPeVH8eN7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "82bccec8",
      "metadata": {
        "id": "82bccec8"
      },
      "source": [
        "Test2: **Your input here!**\n",
        "Remember that \"Your input here!\" means that you can enter your own values or parameters in the code cell below this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ba9cd1",
      "metadata": {
        "id": "e1ba9cd1"
      },
      "outputs": [],
      "source": [
        "# This is a test --> the dash indicates this is a comment within a code cell (check the type above)\n",
        "# Please type your name between the quotation marks replacing the text \"Enter your name here\". Then execute the cell (you can click on \"Run\" in the menu above, or use the cell buttons)\n",
        "my_name = 'Liliana'\n",
        "print(f'Hello {my_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0902ad8",
      "metadata": {
        "id": "a0902ad8"
      },
      "source": [
        "Test3: Insert a cell of the type \"Markdown\" and add some test comments there."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c2aff0c",
      "metadata": {
        "id": "4c2aff0c"
      },
      "source": [
        "Now two sections follow (2 and 3) which have the code that imports the libraries and files necessary for this notebook. Besides code, there is not much to see here since there are no outputs. If you want to skip these parts quickly, go to section 4 (CEN letters overview) and click on Cell -> Run All Above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b6a65b",
      "metadata": {
        "id": "42b6a65b"
      },
      "source": [
        "# 2. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9d38c9",
      "metadata": {
        "id": "7f9d38c9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#source (https://data36.com/plot-histogram-python-pandas/)\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import csv\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from IPython.display import clear_output\n",
        "display(HTML(\"<style>.container { width:65% !important; }</style>\"))\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# to add timestamp to file names\n",
        "import time\n",
        "\n",
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# to visualize networks\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f188da92",
      "metadata": {
        "id": "f188da92"
      },
      "outputs": [],
      "source": [
        "# Marijn's code:\n",
        "def read_data_file(url, password):\n",
        "    response = requests.get(url)\n",
        "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "    csv_filename = zip_file.namelist()[0]\n",
        "    df = pd.read_csv(zip_file.open(csv_filename, 'r', password), sep=',')\n",
        "    # map_binary_columns(df)\n",
        "    # return map_code_type(df)\n",
        "    return df\n",
        "\n",
        "# CURRENT versions (cy61_end)\n",
        "cen_letters_url = \"https://surfdrive.surf.nl/files/index.php/s/qm6s7Q0SAgJ5GIs/download\"\n",
        "cen_unique_persons_url = \"https://surfdrive.surf.nl/files/index.php/s/0o9yV98L8S4O91K/download\"\n",
        "\n",
        "\n",
        "letV01_v00 = read_data_file(cen_letters_url, b'persistence')\n",
        "uniquePersV01_t00 = read_data_file(cen_unique_persons_url, b'persistence')\n",
        "\n",
        "letV01_v00.shape, uniquePersV01_t00.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d80c14",
      "metadata": {
        "id": "53d80c14"
      },
      "source": [
        "# 3. Import & prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d59bf3",
      "metadata": {
        "id": "18d59bf3"
      },
      "source": [
        "## Letters file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc41291",
      "metadata": {
        "id": "abc41291"
      },
      "outputs": [],
      "source": [
        "letV01_v00.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e26789e",
      "metadata": {
        "id": "7e26789e"
      },
      "outputs": [],
      "source": [
        "# Select only the columns that will be offered with the final version of the dataset\n",
        "\n",
        "letV01_v01 = letV01_v00[['SKletterId',\n",
        "'CENRecordID',\n",
        "'epistolariumLetterId_inferredSK',\n",
        "'emloLetterId_inferredSK',\n",
        "'titleOriginal',\n",
        "'ISBD_OriginalCEN',\n",
        "'AFZENDER_OriginalCEN',\n",
        "'SKpersonId_sender',\n",
        "'personStrId_sender',\n",
        "'isUncertainAfzender',\n",
        "'ONTVANGER_OriginalCEN',\n",
        "'SKpersonId_receiver',\n",
        "'personStrId_receiver',\n",
        "'isUncertainOntvanger',\n",
        "'YEAR_Original',\n",
        "'yearItem_start',\n",
        "'yearItem_end',\n",
        "'isYearRange',\n",
        "'yearRangeType',\n",
        "'isYearRangeInferredSk',\n",
        "'isYearItemInferredSk',\n",
        "'isYearItemUncertain',\n",
        "'isYearItemProblematic',\n",
        "'yearItemNotesSk',\n",
        "'DATERING_OriginalCEN',\n",
        "'dayDatingSk',\n",
        "'monthDatingSk',\n",
        "'isDateringUncertain',\n",
        "'AANTAL_OriginalCEN',\n",
        "'quantity_CleanedSk',\n",
        "'isQuantityUncertain',\n",
        "'quantity_BinaryTypeSk',\n",
        "'quantity_DocumentTypeSk',\n",
        "'PLAATS_OriginalCEN',\n",
        "'placeSk',\n",
        "'geonamesIDs',\n",
        "'isPlaatsParsedSk',\n",
        "'isPlaatsDoubtfulSk',\n",
        "'isPlaatsUnclear_CENMark',\n",
        "'isPlaatsMultiple',\n",
        "'plaatsTypeSk',\n",
        "'TAAL_OriginalCEN',\n",
        "'Taal_CleanedSk',\n",
        "'SIGNATUUR_OriginalCEN',\n",
        "'Signatuur_SplitLibrarySK',\n",
        "'Signatuur_SplitFileNumSK',\n",
        "'SignatureShortForMappings',\n",
        "'ANNOTATIE_OriginalCEN',\n",
        "'AnnotatieType_Sk',\n",
        "'X1700_OriginalCEN',\n",
        "'HERKOMST_OriginalCEN',\n",
        "'GeneralNotes_Skillnet',\n",
        "'URLOrLocator'\n",
        "]].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b385fb",
      "metadata": {
        "id": "d8b385fb"
      },
      "outputs": [],
      "source": [
        "# convert datatypes and fill in empty values\n",
        "\n",
        "letV01_v01_columns = letV01_v01.columns\n",
        "\n",
        "for column in letV01_v01:\n",
        "    dataType = letV01_v01.dtypes[column]\n",
        "    if dataType == np.float64:\n",
        "        letV01_v01[column] = letV01_v01[column].fillna(0.0)\n",
        "        letV01_v01[column] = letV01_v01[column].astype(int)\n",
        "    if dataType == object:\n",
        "        letV01_v01[column] = letV01_v01[column].fillna('null')\n",
        "        letV01_v01[column] = letV01_v01[column].astype(str)\n",
        "\n",
        "letV01 = letV01_v01.reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2463ca9",
      "metadata": {
        "id": "d2463ca9"
      },
      "outputs": [],
      "source": [
        "# letV01.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f736d4c",
      "metadata": {
        "id": "2f736d4c"
      },
      "source": [
        "## UniquePersons file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dcbc3b5",
      "metadata": {
        "id": "4dcbc3b5"
      },
      "outputs": [],
      "source": [
        "# uniquePersV01_t00.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf6ad5e",
      "metadata": {
        "id": "caf6ad5e"
      },
      "outputs": [],
      "source": [
        "# Select only the columns that will be offered with the final version of the dataset\n",
        "\n",
        "uniquePersV01_t01 = uniquePersV01_t00[['SKpersonId',\n",
        "'epistolariumPersonId',\n",
        "'magliaOCRPersonId', # change column name to \"garfagnini\"\n",
        "'bellePersonId',\n",
        "'willemCorrPersonId',\n",
        "'CERLpersonId',\n",
        "'BIOPORTpersonId',\n",
        "'EMLOpersonId',\n",
        "'WIKIDATApersonId',\n",
        "'CENPersonIds',\n",
        "'nameType',\n",
        "'personStrId',\n",
        "'nameString',\n",
        "'dateBirth',\n",
        "'dateDeath',\n",
        "'dateFl',\n",
        "'rolesOccupationsTitles',\n",
        "'bioInfo',\n",
        "'alternativeNames',\n",
        "'AFZENDER_ONTVANGER_OriginalCEN',\n",
        "'isUncertain_name',\n",
        "'isUncertain_dateBirth',\n",
        "'isAssigned_dateBirth',\n",
        "'isUncertain_dateDeath',\n",
        "'isAssigned_dateDeath',\n",
        "'isRange_dateFl',\n",
        "'isUncertain_dateFl',\n",
        "'isAssigned_dateFl',\n",
        "'notesDatesPersonv1'\n",
        "]].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e378637",
      "metadata": {
        "id": "9e378637"
      },
      "outputs": [],
      "source": [
        "# convert datatypes and fill in empty values\n",
        "# convert and fill in depending on original data type\n",
        "uniquePersV01_t01_columns = uniquePersV01_t01.columns\n",
        "for column in uniquePersV01_t01:\n",
        "    dataType = uniquePersV01_t01.dtypes[column]\n",
        "    if dataType == np.float64:\n",
        "        uniquePersV01_t01[column] = uniquePersV01_t01[column].fillna(0.0)\n",
        "        uniquePersV01_t01[column] = uniquePersV01_t01[column].astype(int)\n",
        "    if dataType == object:\n",
        "        uniquePersV01_t01[column] = uniquePersV01_t01[column].fillna('null')\n",
        "        uniquePersV01_t01[column] = uniquePersV01_t01[column].astype(str)\n",
        "        \n",
        "# Handle exceptions (I have to do this conversion because integers don't aggregate when groupping in the next steps)\n",
        "uniquePersV01_t01['BIOPORTpersonId'] = uniquePersV01_t01['BIOPORTpersonId'].astype(str)\n",
        "uniquePersV01_t01['EMLOpersonId'] = uniquePersV01_t01['EMLOpersonId'].astype(str)\n",
        "\n",
        "uniquePersV01 = uniquePersV01_t01.reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c8de64",
      "metadata": {
        "id": "48c8de64"
      },
      "outputs": [],
      "source": [
        "# uniquePersV01.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59e45c1",
      "metadata": {
        "id": "c59e45c1"
      },
      "source": [
        "## AllPersons format (processing)\n",
        "Here the file in the format Letters gets converted to the format AllPersons (with Letter Ids) to connect the persons to the letters (via the SKpersonId)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148c3f99",
      "metadata": {
        "id": "148c3f99"
      },
      "outputs": [],
      "source": [
        "# Select only the columns that will be used for the counting of letters\n",
        "\n",
        "allPersAV01_t00 = letV01_v00[['SKletterId',\n",
        "'AFZENDER_OriginalCEN',\n",
        "'SKpersonId_sender',\n",
        "'personStrId_sender',\n",
        "'isUncertainAfzender',\n",
        "'ONTVANGER_OriginalCEN',\n",
        "'SKpersonId_receiver',\n",
        "'personStrId_receiver',\n",
        "'isUncertainOntvanger'\n",
        "]].copy()\n",
        "\n",
        "allPersAV01_t01 = allPersAV01_t00.reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d8b2a8",
      "metadata": {
        "id": "83d8b2a8"
      },
      "outputs": [],
      "source": [
        "# Create Df for Afzenders\n",
        "dfAfzenders = allPersAV01_t01.copy()\n",
        "# drop columns that belong to \"ontvanger\"\n",
        "dfAfzendersOnly = dfAfzenders.drop(['SKpersonId_receiver', 'personStrId_receiver', 'ONTVANGER_OriginalCEN', 'isUncertainOntvanger'], axis=1)\n",
        "# create Id for the person in relation to role (to bring persons data back and forth)\n",
        "dfAfzendersOnly['SKletterIdWithRole'] = dfAfzendersOnly['SKletterId'] + '_afzender'\n",
        "# # rename columns\n",
        "dfAfzendersOnly.rename(columns={'SKpersonId_sender':'SKpersonId', 'personStrId_sender':'personStrId', 'AFZENDER_OriginalCEN':'AFZENDER_ONTVANGER_OriginalCEN', 'isUncertainAfzender':'isUncertainCorrespondent'},inplace=True)\n",
        "# dfAfzendersOnly.info()\n",
        "\n",
        "# Create Df for Ontvangers\n",
        "dfOntvangers = allPersAV01_t01.copy()\n",
        "# drop columns that belong to \"afzender\"\n",
        "dfOntvangersOnly = dfOntvangers.drop(['SKpersonId_sender', 'personStrId_sender', 'AFZENDER_OriginalCEN', 'isUncertainAfzender'], axis=1)\n",
        "# create Id for the user in relation to role (to bring persons data back and forth)\n",
        "dfOntvangersOnly['SKletterIdWithRole'] = dfOntvangersOnly['SKletterId'] + '_ontvanger'\n",
        "# rename columns\n",
        "dfOntvangersOnly.rename(columns={'SKpersonId_receiver':'SKpersonId', 'personStrId_receiver':'personStrId', 'ONTVANGER_OriginalCEN':'AFZENDER_ONTVANGER_OriginalCEN', 'isUncertainOntvanger':'isUncertainCorrespondent'},inplace=True)\n",
        "# dfOntvangersOnly.info()\n",
        "\n",
        "# merge the two sets\n",
        "allPersAV01_t02 = dfAfzendersOnly.append(dfOntvangersOnly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa99865",
      "metadata": {
        "id": "0aa99865"
      },
      "outputs": [],
      "source": [
        "# allPersAV01_t02.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a0c4f1e",
      "metadata": {
        "id": "0a0c4f1e"
      },
      "outputs": [],
      "source": [
        "# if it's all Ok, make a copy:\n",
        "allPersAV01_t03 = allPersAV01_t02.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c5eff6",
      "metadata": {
        "id": "49c5eff6"
      },
      "outputs": [],
      "source": [
        "# convert datatypes and fill in empty values\n",
        "\n",
        "allPersAV01_t03_columns = allPersAV01_t03.columns\n",
        "\n",
        "for column in allPersAV01_t03:\n",
        "    dataType = allPersAV01_t03.dtypes[column]\n",
        "    if dataType == np.float64:\n",
        "        allPersAV01_t03[column] = allPersAV01_t03[column].fillna(0.0)\n",
        "        allPersAV01_t03[column] = allPersAV01_t03[column].astype(int)\n",
        "    if dataType == object:\n",
        "        allPersAV01_t03[column] = allPersAV01_t03[column].fillna('null')\n",
        "\n",
        "# make a copy:\n",
        "allPersAV01 = allPersAV01_t03.reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba757d26",
      "metadata": {
        "id": "ba757d26"
      },
      "outputs": [],
      "source": [
        "# allPersAV01.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229c396d",
      "metadata": {
        "id": "229c396d"
      },
      "source": [
        "## AllPersons format (combined with uniquePersons) (processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ddda99",
      "metadata": {
        "id": "55ddda99"
      },
      "outputs": [],
      "source": [
        "# combine letters metadata with persons metadata (uniquePersons)\n",
        "allPersonsB_t00 = allPersAV01.merge(uniquePersV01, on='SKpersonId', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e08017e2",
      "metadata": {
        "id": "e08017e2"
      },
      "outputs": [],
      "source": [
        "# allPersonsB_t00.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b50a90",
      "metadata": {
        "id": "88b50a90"
      },
      "outputs": [],
      "source": [
        "# drop non-relevant columns generated during merge\n",
        "allPersonsB_t01 = allPersonsB_t00.drop(['personStrId_y', 'AFZENDER_ONTVANGER_OriginalCEN_y'], axis=1)\n",
        "# rename columns\n",
        "allPersonsB_t01.rename(columns={\"personStrId_x\":\"personStrId\", \"AFZENDER_ONTVANGER_OriginalCEN_x\": \"AFZENDER_ONTVANGER_OriginalCEN\"},inplace=True)\n",
        "# allPersonsB_t01.head(5)\n",
        "allPersonsB = allPersonsB_t01.reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add date of last update\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "last_update = timestr\n",
        "last_update"
      ],
      "metadata": {
        "id": "5eihLjuXKypT"
      },
      "id": "5eihLjuXKypT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d9d705d",
      "metadata": {
        "id": "4d9d705d"
      },
      "source": [
        "# 4. CEN Letters overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c54bfd",
      "metadata": {
        "id": "b1c54bfd"
      },
      "source": [
        "This section includes explanations about the CEN dataset and its structure, and the counts of letters, letters per year, and records per archive. It also walks you through some of the challenges of the cleaning process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "535b7f7b",
      "metadata": {
        "id": "535b7f7b"
      },
      "source": [
        "## CEN dataset description"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a62fd6d3",
      "metadata": {
        "id": "a62fd6d3"
      },
      "source": [
        "The CEN dataset is in a tabular format (it is a table with rows and columns where the columns represent the metadata fields (e.g., title, language), and the rows represent the records. The data is in csv format (comma separated).\n",
        "\n",
        "How many records(rows)? Each row in the CEN dataset represents a \"metadata record\", which describes either one or multiple letters. Thus, we cannot say that the number of rows is equal to the number of letters (more on this later), but here below you will see how many rows = records are in the dataset. Remember that we are referring to the slice of the bigger CEN that was cleaned during the SKILLNET project, as described in the introduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf1de4c8",
      "metadata": {
        "id": "bf1de4c8"
      },
      "outputs": [],
      "source": [
        "# rows in the CEN letters dataset\n",
        "number_rows = len(letV01.index)\n",
        "number_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb113c19",
      "metadata": {
        "id": "eb113c19"
      },
      "source": [
        "How many columns? (i.e., what metadata is available for the letters?)\n",
        "\n",
        "The original columns from the CEN dataset (as they were given from OCLC/KB) are kept in the dataset. The initial data dump had these columns:\n",
        "\n",
        "- seqnr,ID,isbd,Jaar,Taal,Afzender,Signatuur,Titel,Aantal,Datering,Ontvanger,Plaats,X1700,Herkomst,Annotatie\n",
        "\n",
        "In the cleaning process undertaken by Skillnet, we created two types of files:\n",
        "- One file for the letter's metadata (which we describe in this section)\n",
        "- One file for the person's metadata (which we describe in the next section)\n",
        "\n",
        "In both cases, additional columns were added, which were necessary for splitting the data in a way that was structured and normalized to facilitate consultation and network analysis research. In any case, the original data was preserved by having together the original column and the \"cleaned\" column(s) next to each other. For example, the original column \"Afzender\" was renamed as 'AFZENDER_OriginalCEN' and, next to it, there are the columns added by Skillnet: \"SKpersonId_sender\" (a person Id that we added) and \"personStrId_sender\" (the normalized form of the name with dates of birth, death and/or floriat) and \"isUncertainAfzender\" (to register uncertainty marks). More examples follow next.\n",
        "\n",
        "While the original dataset had 14 columns, the cleaned version has more columns including the original data columns, the cleaned data columns, and extra columns for control purposes and to register uncertainty in a controlled way (e.g., 'isYearRange', 'isDateringUncertain')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af82d7c4",
      "metadata": {
        "id": "af82d7c4"
      },
      "outputs": [],
      "source": [
        "# columns in the CEN letters dataset\n",
        "number_columns = len(letV01.columns)\n",
        "number_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc7b011",
      "metadata": {
        "id": "2fc7b011"
      },
      "outputs": [],
      "source": [
        "# Column names that will be offered with the final version of the dataset\n",
        "(letV01.columns).to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d972209a",
      "metadata": {
        "id": "d972209a"
      },
      "source": [
        "## How many letters?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cde49fe",
      "metadata": {
        "id": "9cde49fe"
      },
      "source": [
        "The original dataset has a column called \"aantal\" (here named as \"AANTAL_OriginalCEN\") in which cataloguers use, among other terms, the words \"brief\" and \"brieven\" to refer to either one single letter or to a bunch of letters, respectively. The original metadata had also notes added to these descriptions (e.g., digitized, number of folios, etc.)\n",
        "Here below you can take a look to how the original metadata looked like before cleaning this column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7fe298",
      "metadata": {
        "id": "fa7fe298"
      },
      "outputs": [],
      "source": [
        "# unique values in the original CEN metadata for \"aantal\"\n",
        "letters_amount_df = letV01[['AANTAL_OriginalCEN', 'quantity_BinaryTypeSk','quantity_CleanedSk', 'quantity_DocumentTypeSk']]\n",
        "letters_original_amount_count = letters_amount_df.AANTAL_OriginalCEN.value_counts()\n",
        "letters_original_amount_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de8ad0c",
      "metadata": {
        "id": "6de8ad0c"
      },
      "source": [
        "Thus, at SKILLNET we cleaned the \"aantal\" column by dividing the original information into two columns:\n",
        "\n",
        "a) The type (\"brief\" or \"brieven\"), the column is named \"quantity_BinaryTypeSk\"\n",
        "  - We classified as \"brieven\" all records that had a number bigger than \"1\" accompanied with the word \"brieven\". In some cases, when there was a higher number than 1 and the word was \"brief\", we also changed this to \"brieven\". We also took into consideration the \"date\" field to decide whether a record was a \"brief\" or \"brieven\", checking if there was a single year (then it would be \"brief\") or multiple years or a year range (in that case it will be \"brieven\"). The title of the record also had some words that could indicate if it was a \"brief\" or multiple letters.\n",
        "\n",
        "b) The amount (using 1 for \"brief\", and the number of \"brieven\")\n",
        "  - We added \"1\" to all records that were classified as \"brief\". For \"brieven\" we added the number of letters if this one was indicated. We judged every case in which there was a number accompanied by another word than \"brieven\" (e.g., 33 dozen) to decide which number to add. In this case of the 33 boxes, we classified the record as \"brieven\" but added a \"0\" to the amount of brieven, because it is not possible to know how many letters are in those 33 boxes. Zero, then, represents \"undetermined\".\n",
        "  \n",
        "Here we see the approximate number of RECORDS in the CEN dataset that was curated by SKILLNET per type:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DmdCtM-4f1uQ"
      },
      "id": "DmdCtM-4f1uQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9836da99",
      "metadata": {
        "id": "9836da99"
      },
      "outputs": [],
      "source": [
        "# number of records (=rows) that contain either one single letter or multiple letters\n",
        "# excluding the \"brieven\" which amount was \"undetermined\"\n",
        "letters_amount_type_counts = letters_amount_df.quantity_BinaryTypeSk.value_counts()\n",
        "letters_amount_type_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78e2f7c8",
      "metadata": {
        "id": "78e2f7c8"
      },
      "source": [
        "If we sum the values, we get an approximate number of letters here below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb117e4",
      "metadata": {
        "id": "0eb117e4"
      },
      "outputs": [],
      "source": [
        "# approximate number of letters in the CEN dataset (years 1200 to 1820)\n",
        "# summing the number of \"brieven\" in the column \"quantity_CleanedSk\"\n",
        "letters_amount_count = letters_amount_df.quantity_CleanedSk.sum()\n",
        "letters_amount_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9acac6d",
      "metadata": {
        "id": "c9acac6d"
      },
      "source": [
        "But it will never be possible to know the exact amount of letters in the CEN dataset, because there are rows of the type \"brieven\" which don't include the specific number of letters, but descriptions of undertermined bunches, such as \"1 doos\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ca995a",
      "metadata": {
        "id": "a9ca995a"
      },
      "outputs": [],
      "source": [
        "# Here we can see how many records of the type \"brieven\" had an \"undetermined\" value\n",
        "brieven_undetermined = letters_amount_df[(letters_amount_df.quantity_BinaryTypeSk == 'brieven') & (letters_amount_df.quantity_CleanedSk == 0)]\n",
        "len(brieven_undetermined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00a6737",
      "metadata": {
        "id": "f00a6737"
      },
      "outputs": [],
      "source": [
        "# Here we can see which types of containers hold those letters:\n",
        "containers_brieven = brieven_undetermined.AANTAL_OriginalCEN.value_counts()\n",
        "containers_brieven"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b752df",
      "metadata": {
        "id": "00b752df"
      },
      "source": [
        "**Your input here!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780b7f2c",
      "metadata": {
        "id": "780b7f2c"
      },
      "outputs": [],
      "source": [
        "# Try it yourself: find, for example, how many times cataloguers used words for \"box\" in the description:\n",
        "search1_df = letters_amount_df[letters_amount_df.AANTAL_OriginalCEN.str.contains('dozen|doos')]\n",
        "search1_df.AANTAL_OriginalCEN.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524ddabb",
      "metadata": {
        "id": "524ddabb"
      },
      "source": [
        "**Your input here!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf56d16",
      "metadata": {
        "id": "bcf56d16"
      },
      "outputs": [],
      "source": [
        "# Do you want to see the letters that have a specific amount in the original metadata? change the parameter inside the quotation marks for the value you are interested in. You can use regular expressions\n",
        "# for example: str.contains(r'9 dozen')\n",
        "search2 = letV01[letV01.AANTAL_OriginalCEN.str.contains(r'9 dozen')]\n",
        "search2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c8bbd28",
      "metadata": {
        "id": "6c8bbd28"
      },
      "source": [
        "We also found that other information related to the type of document was added to the title metadata. We extracted that information into a column called \"quantity_DocumentTypeSk\".\n",
        "Here below it's possible to see which types of documents were described (i.e., there are not only letters in the CEN dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64b51a2",
      "metadata": {
        "id": "a64b51a2"
      },
      "outputs": [],
      "source": [
        "# types of documents in the CEN dataset\n",
        "type_document_counts = letters_amount_df.quantity_DocumentTypeSk.value_counts()\n",
        "type_document_counts.sort_index(ascending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86c33648",
      "metadata": {
        "id": "86c33648"
      },
      "source": [
        "**Your input here!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5c6bcd",
      "metadata": {
        "id": "aa5c6bcd"
      },
      "outputs": [],
      "source": [
        "# Do you want to see the letters of a specific document type? change the parameter inside the quotation marks for the value you are interested in. You can use regular expressions\n",
        "# for example: str.contains(r'Opgave|order') where the pipe symbol means \"or\"\n",
        "search3 = letV01[letV01.quantity_DocumentTypeSk.str.contains(r'Opgave|order')]\n",
        "search3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "627be4db",
      "metadata": {
        "id": "627be4db"
      },
      "source": [
        "## From which years?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1735e7b2",
      "metadata": {
        "id": "1735e7b2"
      },
      "source": [
        "The original dataset has a column called \"Jaar\" (which we renamed as \"YEAR_Original\") in which cataloguers entered the date of the letter(s). However, they also added uncertainty marks in many ways (?, ca., 15XX) or ranges (1620-1630). There was also a \"Datering\" column in the original metadata. \n",
        "During the SKILLNET project we progressively cleaned this column and converted it into three columns:\n",
        "- Year: which we converted into two columns: 'yearItem_start' and 'yearItem_end'. If a letter has a single year, it is kept in the column 'yearItem_start'. If the record has a range (either for a bunch of \"letters\" with a start and and end year, or for a specific letter in which the range is used to express uncertainty), both years are registered in the two columns\n",
        "- 'monthDatingSk'\n",
        "- 'dayDatingSk'\n",
        "\n",
        "Besides, we added extra columns to keep stored the uncertainty marks, and to indicate if the year is a range, but in a standardized way. These columns have only two values: \"y\" or \"n\". We indicate here below when the value turns into \"y\":\n",
        "- 'isYearRange' : ditto\n",
        "- 'isYearRangeInferredSk': when, as part of the cleaning process, we inferred the range using, for example, the dates of birth/death of the correspondents\n",
        "- 'isYearRangeUncertain': when the range is used to express uncertainty (e.g., a letter was written between 1610 - 1630).\n",
        "- 'isYearItemInferredSk': when, as part of the research process, one of the experts found out the exact year of a letter (in that case, the range is deleted and replaced for a specific year)\n",
        "- 'isYearItemUncertain': when the specific year of a letter is inferred, thus, it's still uncertain\n",
        "- 'isDateringUncertain': when the cataloguers added any uncertainty mark, or when the datering was inferred at Skillnet but it's still uncertain\n",
        "\n",
        "Here below you can take a look to how the original metadata looked like before cleaning the \"Jaar\" and \"Datering\" columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967e90f2",
      "metadata": {
        "id": "967e90f2"
      },
      "outputs": [],
      "source": [
        "# date-related columns\n",
        "letters_dates_df = letV01[['SKletterId','ISBD_OriginalCEN', 'titleOriginal', 'YEAR_Original', 'yearItem_start','yearItem_end', 'isYearRange', 'yearRangeType', 'isYearRangeInferredSk', 'isYearItemInferredSk', 'isYearItemInferredSk', 'isYearItemUncertain', 'DATERING_OriginalCEN','dayDatingSk', 'monthDatingSk', 'isDateringUncertain']]\n",
        "# date-related columns to display with original data\n",
        "letters_dates_original_df = letters_dates_df[['SKletterId','YEAR_Original', 'DATERING_OriginalCEN']]\n",
        "letters_dates_original_df.head(500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d6a7e4",
      "metadata": {
        "id": "c4d6a7e4"
      },
      "source": [
        "If we would try to group the letters per year using the original \"Jaar\" column as it was, we would obtain an enormous amount of unique values, thus, if you would try to depict this in a graphic, it would be impossible to know how many letters were there per year. Here below you see how many unique values are in the original dataset in that column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdcbb9d8",
      "metadata": {
        "id": "bdcbb9d8"
      },
      "outputs": [],
      "source": [
        "# unique values in the original CEN metadata for \"Jaar\"\n",
        "letters_dates_df.YEAR_Original.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3243ef5f",
      "metadata": {
        "id": "3243ef5f"
      },
      "source": [
        "After cleaning the columns mentioned above, we got a smaller number of unique values per year only, and the months and days were separated into other two different columns. All the uncertainty marks were also added to control columns as mentioned above. Here you can take a look to the current cleaned version of the letter dates with the different columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c3529d",
      "metadata": {
        "id": "04c3529d"
      },
      "outputs": [],
      "source": [
        "# date-related columns to display current cleaned data\n",
        "letters_dates_cleaned_df = letters_dates_df[['SKletterId','yearItem_start', 'yearItem_end', 'isYearRange', 'isYearRangeInferredSk', 'yearRangeType', 'isYearItemInferredSk', 'isYearItemUncertain','dayDatingSk', 'monthDatingSk', 'isDateringUncertain']]\n",
        "letters_dates_cleaned_df.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d35906a",
      "metadata": {
        "id": "7d35906a"
      },
      "source": [
        "The number of unique values is reduced from several thousands to a few hundreds (for the years only):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35ce0b7",
      "metadata": {
        "id": "f35ce0b7"
      },
      "outputs": [],
      "source": [
        "# number of unique values in the cleaned column that contains the letter year\n",
        "letters_dates_cleaned_df.yearItem_start.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524b2ea0",
      "metadata": {
        "id": "524b2ea0"
      },
      "source": [
        "But not all letter dates are certain. As we mentioned, there were uncertainty marks added by the cataloguers, or some dates are not known. In some cases, we inferred the letter year(s) using an estimated range based on the dates of birth or death of the correspondents. If we want to represent the distribution of letters per year, we have to take that into account. Here below we explain in detail what the \"certain\" years and the \"uncertain\" years mean:\n",
        "\n",
        "1. certain years include:\n",
        "- a) the letters that have specific years and these years are certain.\n",
        "- b) when the CEN letter record was used to describe not only one letter but several letters. E.g., one row is to describe 60 letters, which were written between 1688 and 1680. In this case, the range doesn't indicate uncertainty, but an actual period of time in which several letters were exchanged.\n",
        "\n",
        "2. uncertain years include:  \n",
        "- c) A letter may have a specific year, but this is uncertain (as indicated by a question mark or any other uncertainty mark in the original metadata, e.g., 1520?).\n",
        "- d) It can also happen that uncertainty was indicated by adding a range (e.g., a letter was written in some year between 1688 and 1680). \n",
        "- e) Several letters described with a range (as above) may also be uncertain or incorrect. For example, in this CEN record (https://picarta.oclc.org/psi/DB=3.23/XMLPRS=Y/PPN?PPN=310885922) the range of letter exchange indicated by the cataloguer was 1641-1674 (for 3 letters). However, the dates of birth and death of the correspondents let us see that there is a mistake in that range, since one of the correspondents was already dead in the ending year of the range. For these items, we have added a \"y\" to the column \"isYearItemUncertain\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58bb0fe9",
      "metadata": {
        "id": "58bb0fe9"
      },
      "source": [
        "Before selecting the letters with certain years, and to make it easier to plot the distribution of letters along time, we will convert the individual years into decades:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd54720a",
      "metadata": {
        "id": "dd54720a"
      },
      "outputs": [],
      "source": [
        "# turning years into decades\n",
        "letters_dates_df_v2 = letters_dates_df.reset_index(drop=True).copy()\n",
        "letters_dates_df_v2['decade'] = letters_dates_df_v2['yearItem_start'] // 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c9b566",
      "metadata": {
        "id": "b6c9b566"
      },
      "source": [
        "In order to represent the distribution of letters per year, we only take the subset explained under a) above. We exclude the rest for this visualization purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d19ff9b",
      "metadata": {
        "id": "1d19ff9b"
      },
      "outputs": [],
      "source": [
        "# first criterium: letter has specific years (no ranges) --> isYearRange == 'n' and the specific year of the letter is not uncertain --> isYearItemUncertain == 'n'\n",
        "# second criterium: letter has a range --> isYearRange == 'y' and this range is not uncertain --> yearRangeType == 'n'\n",
        "\n",
        "letters_dates_certain_df = letters_dates_df_v2[((letters_dates_df_v2.isYearRange == 'n') & (letters_dates_df_v2.isYearItemUncertain == 'n')) | ((letters_dates_df_v2.isYearRange == 'y') & (letters_dates_df_v2.yearRangeType == 'bunchOfLetters_certainRange'))]\n",
        "\n",
        "# display number of rows (records) which fulfill those conditions\n",
        "letters_dates_certain_df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0c544a0",
      "metadata": {
        "id": "c0c544a0"
      },
      "source": [
        "Now we can plot the distribution of letters with certain years (of type a: certain years for individual letters) per decade:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0859c856",
      "metadata": {
        "id": "0859c856"
      },
      "outputs": [],
      "source": [
        "# plot the letters per decade using only the letters that have \"certain\" years\n",
        "ax = letters_dates_certain_df.decade.value_counts().sort_index().plot(kind=\"bar\", rot=90, figsize=(20, 10))\n",
        "ax.set_title(\"Distribution of letters per decade in the CEN dataset (1200-1820), non-dated letters excluded\")\n",
        "ax.set_xlabel(\"Decades\")\n",
        "ax.set_ylabel(\"Number of letters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09d27def",
      "metadata": {
        "id": "09d27def"
      },
      "source": [
        "But keep in mind that many letters don't have a year. Those letters may either be excluded from the dataset, or, after the persons metadata is more complete, undergo another \"data imputation\" process to get an estimated range using the dates of birth/death of the correspondents (if they exist)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ec5915",
      "metadata": {
        "id": "d3ec5915"
      },
      "outputs": [],
      "source": [
        "# letters with no year\n",
        "letters_no_year = letV01[letV01.yearItem_start == 0]\n",
        "letters_no_year_count = len(letters_no_year.index)\n",
        "percentage = round((letters_no_year_count * 100 / number_rows), 2)\n",
        "print(f'Letters with no year: {letters_no_year_count}, which is {percentage}% of the total records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b770231e",
      "metadata": {
        "id": "b770231e"
      },
      "outputs": [],
      "source": [
        "# see an example of those letters without year\n",
        "letters_no_year.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59134666",
      "metadata": {
        "id": "59134666"
      },
      "source": [
        "## From which archives?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d7fb6ee",
      "metadata": {
        "id": "8d7fb6ee"
      },
      "source": [
        "The original metadata included a field called \"Signatuur\" which corresponds to the code in the filing system of the archive or library that holds the letter. Here below we can see, based on that mark, how many letters are held per each archive/library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa025640",
      "metadata": {
        "id": "fa025640"
      },
      "outputs": [],
      "source": [
        "# because some letters were merged by Skillnet when the record represented the same letter, we grouped in a column the signatuur marks. In order to get the individual values, here below we split them again\n",
        "archives_t0 = pd.DataFrame(letV01.Signatuur_SplitLibrarySK.str.split('|').tolist(), index=letV01.SKletterId).stack()\n",
        "# We now want to get rid of the secondary index\n",
        "archives_t1 = archives_t0.reset_index([0, 'SKletterId'])\n",
        "# Set the column names as we want them\n",
        "archives_t1.columns = ['SKletterId', 'Archive']\n",
        "# Group by archive name and count the number of records\n",
        "archives_count = archives_t1.groupby(['Archive'])['SKletterId'].count().sort_values(ascending=False)\n",
        "archives_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d596a0b",
      "metadata": {
        "id": "2d596a0b"
      },
      "source": [
        "# 5. CEN Persons overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053fe5be",
      "metadata": {
        "id": "053fe5be"
      },
      "source": [
        "## Unique persons (entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8571b3",
      "metadata": {
        "id": "2e8571b3"
      },
      "source": [
        "In the original CEN dataset, the senders and receivers were entered in two different columns (afzender / ontvanger). Because these names were not entered using an authority file, and they were entered by different archives over different years, the values were highly inconsistent. There are all types of issues with these names, which we will illustrate below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15962707",
      "metadata": {
        "id": "15962707"
      },
      "source": [
        "### Original data (sender/receiver)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60f45040",
      "metadata": {
        "id": "60f45040"
      },
      "source": [
        "Here you can take a look into how the original data looked like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7826f98",
      "metadata": {
        "id": "a7826f98"
      },
      "outputs": [],
      "source": [
        "# person-related columns from letters file\n",
        "letters_persons_df = letV01[['SKletterId','ISBD_OriginalCEN', 'titleOriginal', 'AFZENDER_OriginalCEN','isUncertainAfzender', 'ONTVANGER_OriginalCEN', 'isUncertainOntvanger']]\n",
        "# person-related columns to display with original data\n",
        "letters_persons_original_df = letters_persons_df[['SKletterId','ISBD_OriginalCEN', 'titleOriginal', 'AFZENDER_OriginalCEN','ONTVANGER_OriginalCEN']]\n",
        "letters_persons_original_df.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec04dda",
      "metadata": {
        "id": "6ec04dda"
      },
      "source": [
        "This sample of the original data shows already some of the issues, for example:\n",
        "- Missing data: not all letters had a receiver or a sender\n",
        "- Inconsistency in the use of the CEN person Ids: some person names have an Id and some don't. \n",
        "- Some person names are properly identified with dates of birth and death, while others are not.\n",
        "- Some names have additional information (for example, a role) which is part of the name string. This makes it difficult to match the strings with other datasets where only the name is used.\n",
        "- ... etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4640320",
      "metadata": {
        "id": "d4640320"
      },
      "source": [
        "Besides inconsistency, missing data is an important issue. Mostly, organizations were not provided in the original data dump. To solve this issue, we had to extract them ourselves from the title metadata, this was mostly the case for \"ontvangers\". See for example that the receiver is mentioned in the ISBD, but it's not present in the receiver column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3702a861",
      "metadata": {
        "id": "3702a861"
      },
      "outputs": [],
      "source": [
        "missing_receiver = letters_persons_original_df[letters_persons_original_df.ONTVANGER_OriginalCEN == 'null']\n",
        "missing_receiver.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19127bde",
      "metadata": {
        "id": "19127bde"
      },
      "source": [
        "But the most notable issues arise when we put the senders and the receivers together. Here you can take a look to the original data when we join all the person names in a single table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cd77fea",
      "metadata": {
        "id": "0cd77fea"
      },
      "outputs": [],
      "source": [
        "# person-related columns to show original data vs cleaned data\n",
        "persons_original_df = uniquePersV01[['SKpersonId','CENPersonIds','AFZENDER_ONTVANGER_OriginalCEN', 'personStrId']]\n",
        "persons_original = persons_original_df.AFZENDER_ONTVANGER_OriginalCEN.sort_values(ascending=False).value_counts()\n",
        "persons_original.head(500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3665d6f2",
      "metadata": {
        "id": "3665d6f2"
      },
      "source": [
        "There are many aspects to observe in the sample of the original names above, but the most important thing to notice is that most names were NN or empty, and there is a very long tale of names that only occur 2 or only 1 time... In the original dataset there were more than 37.000 unique names, but this doesn't mean that there was such a big amount of persons or entities, they were just not consistently entered..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a21fae2",
      "metadata": {
        "id": "7a21fae2"
      },
      "outputs": [],
      "source": [
        "# Here you can see how many unique values (persons/entities = strings) were in the original data\n",
        "persons_original.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b19f13",
      "metadata": {
        "id": "79b19f13"
      },
      "source": [
        "**Your input here!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617b567b",
      "metadata": {
        "id": "617b567b"
      },
      "outputs": [],
      "source": [
        "# Do you want to see which forms the cataloguers used to enter the name of a person? replace the name below, for example, see the variations\n",
        "# in the name of \"Adrianus Junius\": str.contains(r'.*dr.*Junius') for the name you want to consult\n",
        "search3 = persons_original_df[persons_original_df.AFZENDER_ONTVANGER_OriginalCEN.str.contains(r'.*dr.*Junius')]\n",
        "search3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16613059",
      "metadata": {
        "id": "16613059"
      },
      "source": [
        "As you see in the first line above, in the column \"Afzender_Ontvanger_OriginalCEN\", separated by a pipe symbol (|) there are the different variants of the name in the original metadata. There were 6 forms, some of them had a CEN person Id but some didn't, some had person floriat dates, but others didn't. Most of the normalization process to come up with the conclusion that those 6 forms of the name referred to \"Adriaan Junius (1639-1680)\" were done semi-automatically, but there was also a great deal of manual research involved."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c12743d4",
      "metadata": {
        "id": "c12743d4"
      },
      "source": [
        "### Types of unique entities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bb546c",
      "metadata": {
        "id": "e1bb546c"
      },
      "source": [
        "But in the CEN dataset there were not only persons involved as correspondents, there are also other entities such as organizations or groups. And, in some cases, the names are generic (for example: \"zijn zus\". We classified the types of entities. Here below you can see the types and their amount. The cleaning process up to date (July 7, 2022) has mostly been focused on the \"namedPersons\". Future work includes the cleaning up of the organizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dec8013",
      "metadata": {
        "id": "3dec8013"
      },
      "outputs": [],
      "source": [
        "# types of \"entities\"\n",
        "allPersonsB.nameType.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b5b221",
      "metadata": {
        "id": "06b5b221"
      },
      "source": [
        "## Letters per person/entity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95beb76d",
      "metadata": {
        "id": "95beb76d"
      },
      "source": [
        "In this section we will visualize the distribution of letters per person or entity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80b64e12",
      "metadata": {
        "id": "80b64e12"
      },
      "source": [
        "### Letters per unique senders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ce4b64",
      "metadata": {
        "id": "46ce4b64"
      },
      "source": [
        "Here below you can see a list of the senders ordered per the amount of letters they sent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ce64a1",
      "metadata": {
        "id": "73ce64a1"
      },
      "outputs": [],
      "source": [
        "senders_letters_count_df = allPersAV01[allPersAV01.SKletterIdWithRole.str.contains('_afzender')]\n",
        "senders_letters_count = senders_letters_count_df.groupby(['personStrId'])['SKletterId'].count().reset_index()\n",
        "# sort values\n",
        "sorted_senders_letters_count = senders_letters_count.sort_values(ascending=False,by='SKletterId')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6982b8a",
      "metadata": {
        "id": "e6982b8a"
      },
      "source": [
        "We set it up to display a list with 500 names... If you would like to display a different number, change the value inside the parenthesis after head. If you want to display the entire list, delete this part: .head(500). If you want to display the entire list (be patient!) it should look like this: \n",
        "\n",
        "sorted_senders_letters_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45af4684",
      "metadata": {
        "id": "45af4684"
      },
      "outputs": [],
      "source": [
        "# display the list of senders in descending order of number of letters sent\n",
        "sorted_senders_letters_count.head(500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7010a80",
      "metadata": {
        "id": "c7010a80"
      },
      "outputs": [],
      "source": [
        "# # number of senders\n",
        "# sorted_senders_letters_count.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbaa46c4",
      "metadata": {
        "id": "dbaa46c4"
      },
      "source": [
        "**Your input here!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15b30c71",
      "metadata": {
        "id": "15b30c71"
      },
      "source": [
        "Because we cannot plot all the list with all senders, we have to make a smaller selection. Here below the code is set to show the top 20. You can change that value if you want to show less or more senders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "924fb615",
      "metadata": {
        "id": "924fb615"
      },
      "outputs": [],
      "source": [
        "# determine how many persons should be shown\n",
        "top_s = 20\n",
        "# create small df for displaying and plotting\n",
        "letters_per_sender = sorted_senders_letters_count.head(top_s).reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493cebf4",
      "metadata": {
        "id": "493cebf4"
      },
      "outputs": [],
      "source": [
        "# plotting in a barh chart the top n senders\n",
        "ax = senders_letters_count_df.groupby(['personStrId'])['SKletterId'].count().sort_values(ascending=True).tail(top_s).plot(kind='barh', figsize=(20, 10))\n",
        "ax.set_title(\"Top senders in the CEN dataset (1200-1820)\")\n",
        "ax.set_xlabel(\"Number of letters\")\n",
        "ax.set_ylabel(\"Sender\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d278d13",
      "metadata": {
        "id": "4d278d13"
      },
      "source": [
        "### Letters per unique receivers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a09bbe",
      "metadata": {
        "id": "29a09bbe"
      },
      "source": [
        "Here below you can see a list of the receivers ordered per the amount of letters they received."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07a48b2b",
      "metadata": {
        "id": "07a48b2b"
      },
      "outputs": [],
      "source": [
        "receivers_letters_count_df = allPersAV01[allPersAV01.SKletterIdWithRole.str.contains('_ontvanger')]\n",
        "receivers_letters_count = receivers_letters_count_df.groupby(['personStrId'])['SKletterId'].count().reset_index()\n",
        "# sort values\n",
        "sorted_receivers_letters_count = receivers_letters_count.sort_values(ascending=False,by='SKletterId')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de7a94db",
      "metadata": {
        "id": "de7a94db"
      },
      "source": [
        "We set it up to display a list with 500 names... If you would like to display a different number, change the value inside the parenthesis after head. If you want to display the entire list, delete this part: .head(500). If you want to display the entire list (be patient!) it should look like this: \n",
        "\n",
        "sorted_receivers_letters_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b92526",
      "metadata": {
        "id": "12b92526"
      },
      "outputs": [],
      "source": [
        "sorted_receivers_letters_count.head(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75518103",
      "metadata": {
        "id": "75518103"
      },
      "outputs": [],
      "source": [
        "# # number of receivers\n",
        "# sorted_receivers_letters_count.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c47aff3d",
      "metadata": {
        "id": "c47aff3d"
      },
      "source": [
        "**Your input here!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "591b6892",
      "metadata": {
        "id": "591b6892"
      },
      "source": [
        "Because we cannot plot all the list with all senders, we have to make a smaller selection. Here below the code is set to show the top 20. You can change that value if you want to show less or more senders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7a1fca",
      "metadata": {
        "id": "8c7a1fca"
      },
      "outputs": [],
      "source": [
        "# determine how many persons should be shown\n",
        "top_r = 20\n",
        "# create small df for displaying and plotting\n",
        "letters_per_receiver = sorted_receivers_letters_count.head(top_r).reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7708d23b",
      "metadata": {
        "id": "7708d23b"
      },
      "outputs": [],
      "source": [
        "# plotting in a barh chart the top n receivers\n",
        "ax = receivers_letters_count_df.groupby(['personStrId'])['SKletterId'].count().sort_values(ascending=True).tail(top_r).plot(kind='barh', figsize=(20, 10))\n",
        "ax.set_title(\"Top receivers in the CEN dataset (1200-1820)\")\n",
        "ax.set_xlabel(\"Number of letters\")\n",
        "ax.set_ylabel(\"Receiver\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63111e29",
      "metadata": {
        "id": "63111e29"
      },
      "source": [
        "### Letters per unique entity (person, organization, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "836bb445",
      "metadata": {
        "id": "836bb445"
      },
      "source": [
        "Here below you can see a list of the entities (persons, organizations, etc.) ordered per the amount of letters they appear in as correspondents (sender or receiver)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741b4506",
      "metadata": {
        "id": "741b4506"
      },
      "outputs": [],
      "source": [
        "person_letters_count_df = allPersonsB.copy()\n",
        "person_letters_count = person_letters_count_df.groupby(['personStrId'])['SKletterId'].count().reset_index()\n",
        "# sort values\n",
        "sorted_person_letters_count = person_letters_count.sort_values(ascending=False,by='SKletterId')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d579b0",
      "metadata": {
        "id": "83d579b0"
      },
      "source": [
        "We set it up to display a list with 500 names... If you would like to display a different number, change the value inside the parenthesis after head. If you want to display the entire list, delete this part: .head(500). If you want to display the entire list (be patient!) it should look like this: \n",
        "\n",
        "sorted_person_letters_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c056f7e",
      "metadata": {
        "id": "7c056f7e"
      },
      "outputs": [],
      "source": [
        "sorted_person_letters_count.head(500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d988bf2",
      "metadata": {
        "id": "8d988bf2"
      },
      "source": [
        "**Your input here!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c679be",
      "metadata": {
        "id": "e6c679be"
      },
      "source": [
        "Because we cannot plot all the list with all senders, we have to make a smaller selection. Here below the code is set to show the top 20. You can change that value if you want to show less or more senders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ccb5aa1",
      "metadata": {
        "id": "4ccb5aa1"
      },
      "outputs": [],
      "source": [
        "# determine how many persons should be shown\n",
        "top_p = 20\n",
        "# create small df for displaying and plotting\n",
        "letters_per_person = sorted_person_letters_count.head(top_p).reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e70408",
      "metadata": {
        "id": "77e70408"
      },
      "outputs": [],
      "source": [
        "# plotting in a barh chart the top n correspondents\n",
        "ax = person_letters_plot = person_letters_count_df.groupby(['personStrId'])['SKletterId'].count().sort_values(ascending=True).tail(top_p).plot(kind='barh', figsize=(20, 10))\n",
        "ax.set_title(\"Top correspondents in the CEN dataset (1200-1820)\")\n",
        "ax.set_xlabel(\"Number of letters\")\n",
        "ax.set_ylabel(\"Correspondent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6b5799",
      "metadata": {
        "id": "eb6b5799"
      },
      "source": [
        "## Relations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa164e14",
      "metadata": {
        "id": "aa164e14"
      },
      "source": [
        "Because of the size of the CEN dataset, and due to the fact that it includes letters from different archives and persons, the opportunities for network analysis that it offers are unique. In this section we will explore the first-degree network of a person of interest (for you to select) and also visualize the second-degree network of that person."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0bd78c",
      "metadata": {
        "id": "fe0bd78c"
      },
      "source": [
        "### First-degree network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690f1bd6",
      "metadata": {
        "id": "690f1bd6"
      },
      "source": [
        "Based on the letters file, it's possible to generate a list of the persons a correspondent kept direct contact with (the first-degree network). The format that we have generated has the list per each person in the dataset. However, it would be easier to see the network of a person you are interested in. Thus, this is done in two steps:\n",
        "- First: consult which is the Id of the person you are interested by quering per name\n",
        "- Second: enter the Id that you found in the code that generates the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2c4a1f",
      "metadata": {
        "id": "bb2c4a1f"
      },
      "outputs": [],
      "source": [
        "# create a small df to query for the Id of a person using it's name or date of birth, death or floriat\n",
        "basicInfo_person_df = uniquePersV01[['SKpersonId', 'personStrId', 'dateBirth', 'dateDeath', 'dateFl']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a369d9",
      "metadata": {
        "id": "58a369d9"
      },
      "source": [
        "**Your input!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f14a5a",
      "metadata": {
        "id": "15f14a5a"
      },
      "source": [
        "The purpose of the cell below is to query the list of persons in the dataset and obtain his/her Id. The Id is used to query the letter dataset to find the first-degree contacts.\n",
        "You can also use regular expressions to query for a name (that is the meaning of the letter \"r\" inside the parenthesis. For example: \n",
        "- because 'Leeuwenhoek' may be written in different ways, you could use: (r'Ant.*Lee.*') --> this means that the name starts with Ant and the last name starts with Lee, but you are not sure of what the words after are. Please notice that it is case sensitive\n",
        "- if you hesitate between two variants, for example Adriaan and Adrianus, and you know the last name, you can use or (the pipe symbol): (r'Adriaan.*Junius|Adrianus.*Junius'). Remember that a dot represents any character, and an asteriks represents 1 or more times. Then, we are looking for any characters between the name and the last name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e26ead0",
      "metadata": {
        "id": "3e26ead0"
      },
      "outputs": [],
      "source": [
        "# First step: query using the name of the person you are interested in and copy the SKpersonId\n",
        "person_of_interest = basicInfo_person_df[basicInfo_person_df.personStrId.str.contains(r'Brunetti')] # I selected here a person with a small network for display purposes in Collab (slow)\n",
        "person_of_interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8582726",
      "metadata": {
        "id": "f8582726"
      },
      "outputs": [],
      "source": [
        "# Second step: paste the person Id below (don't forget to paste it two times!):\n",
        "firstDegree_df = letV01[(letV01.SKpersonId_sender.str.contains('skp009119')) | (letV01.SKpersonId_receiver.str.contains('skp009119'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "266032ff",
      "metadata": {
        "id": "266032ff"
      },
      "outputs": [],
      "source": [
        "# Display the letters in the first-degree network of a person of interest (remember to change the number within head(#) if you want to see more)\n",
        "firstDegree_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2972f273",
      "metadata": {
        "id": "2972f273"
      },
      "outputs": [],
      "source": [
        "# generate df with unique first-degree corespondents (SKpersonId + personStrId)\n",
        "correspondents_df1 = firstDegree_df[['SKpersonId_sender','personStrId_sender', 'SKpersonId_receiver', 'personStrId_receiver']]\n",
        "firstDegree_senders = correspondents_df1[['SKpersonId_sender','personStrId_sender']].copy()\n",
        "firstDegree_senders.rename(columns={'SKpersonId_sender':'SKpersonId', 'personStrId_sender':'personStrId'},inplace=True)\n",
        "firstDegree_receivers = correspondents_df1[['SKpersonId_receiver','personStrId_receiver']].copy()\n",
        "firstDegree_receivers.rename(columns={'SKpersonId_receiver':'SKpersonId', 'personStrId_receiver':'personStrId'},inplace=True)\n",
        "firstDegree_append = firstDegree_senders.append(firstDegree_receivers)\n",
        "firstDegree_unique = firstDegree_append.drop_duplicates()\n",
        "firstDegree = firstDegree_unique.reset_index(drop=True)\n",
        "firstDegree.sort_values(by='personStrId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4972676e",
      "metadata": {
        "id": "4972676e"
      },
      "outputs": [],
      "source": [
        "# How big is the first-degree network?\n",
        "firstDegree.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ce427f",
      "metadata": {
        "id": "52ce427f"
      },
      "source": [
        "### Second-degree network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598fdc1c",
      "metadata": {
        "id": "598fdc1c"
      },
      "source": [
        "Visualize a first-degree network is not very interesting. Thus, we will capture now the second-degree network and represent it visually. A second degree network consist of the first-degree correspondents of each person above. We will then combine the first and second-degree network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d97d1a73",
      "metadata": {
        "id": "d97d1a73"
      },
      "outputs": [],
      "source": [
        "# make a copy of the letters df\n",
        "letters_secondDegree_df = letV01.copy()\n",
        "# do an inner merge of the senders with the list of first-degree correspondents\n",
        "letters_2ndD_senders_df = letters_secondDegree_df.merge(firstDegree, left_on='SKpersonId_sender', right_on='SKpersonId', how='inner')\n",
        "# do an inner merge of the receivers with the list of first-degree correspondents\n",
        "letters_2ndD_receivers_df = letters_secondDegree_df.merge(firstDegree, left_on='SKpersonId_receiver', right_on='SKpersonId', how='inner')\n",
        "# to get the letters df I need to append and drop duplicate rows\n",
        "letters_2ndD_append_df = letters_2ndD_senders_df.append(letters_2ndD_receivers_df)\n",
        "letters_2ndD_unique = letters_2ndD_append_df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454942a4",
      "metadata": {
        "id": "454942a4"
      },
      "outputs": [],
      "source": [
        "# display the number of letters in the second-degree network dataset\n",
        "letters_2ndD_unique.shape\n",
        "# letters_2ndD_unique.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2832fdc1",
      "metadata": {
        "id": "2832fdc1"
      },
      "outputs": [],
      "source": [
        "# generate df with unique second-degree corespondents (SKpersonId + personStrId)\n",
        "correspondents_df2 = letters_2ndD_unique[['SKpersonId_sender','personStrId_sender', 'SKpersonId_receiver', 'personStrId_receiver']]\n",
        "secondDegree_senders = correspondents_df2[['SKpersonId_sender','personStrId_sender']].copy()\n",
        "secondDegree_senders.rename(columns={'SKpersonId_sender':'SKpersonId', 'personStrId_sender':'personStrId'},inplace=True)\n",
        "secondDegree_receivers = correspondents_df2[['SKpersonId_receiver','personStrId_receiver']].copy()\n",
        "secondDegree_receivers.rename(columns={'SKpersonId_receiver':'SKpersonId', 'personStrId_receiver':'personStrId'},inplace=True)\n",
        "secondDegree_append = secondDegree_senders.append(secondDegree_receivers)\n",
        "secondDegree_unique = secondDegree_append.drop_duplicates()\n",
        "secondDegree = secondDegree_unique.reset_index(drop=True)\n",
        "secondDegree.sort_values(by='personStrId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9e08b1",
      "metadata": {
        "id": "5b9e08b1"
      },
      "outputs": [],
      "source": [
        "# display how big is the secon-degree network\n",
        "secondDegree.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bfdf533",
      "metadata": {
        "id": "9bfdf533"
      },
      "source": [
        "To represent the second-degree network visually, we need a format like this:\n",
        "Source,Target,Type,Weight\n",
        "Thus, we now convert our second-degree letter dataset to that format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab7d660b",
      "metadata": {
        "id": "ab7d660b"
      },
      "outputs": [],
      "source": [
        "letters_2ndD_viz_t01 = letters_2ndD_unique[['personStrId_sender', 'personStrId_receiver']]\n",
        "letters_2ndD_viz_t02 = letters_2ndD_viz_t01.reset_index(drop=True).copy()\n",
        "letters_2ndD_viz_t02['Type'] = 'Undirected'\n",
        "letters_2ndD_viz_t02['weight'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd9f91c",
      "metadata": {
        "id": "efd9f91c"
      },
      "outputs": [],
      "source": [
        "letters_2ndD_viz_t02.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebab58a",
      "metadata": {
        "id": "bebab58a"
      },
      "outputs": [],
      "source": [
        "letters_2ndD_viz_t02.rename(columns={'personStrId_sender':'Source', 'personStrId_receiver':'Target'},inplace=True)\n",
        "secDeg_df = letters_2ndD_viz_t02.reset_index(drop=True).copy()\n",
        "# secDeg_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f11c15a",
      "metadata": {
        "id": "8f11c15a"
      },
      "source": [
        "You may want to see how many letters are in the second-degree network dataset, here below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8570568b",
      "metadata": {
        "id": "8570568b"
      },
      "outputs": [],
      "source": [
        "# number of letters in the second-degree dataset of the person of interest\n",
        "secDeg_df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c5cdb5b",
      "metadata": {
        "id": "1c5cdb5b"
      },
      "source": [
        "**Disclaimer**: This section is an experiment, the visualization library is not fully used as it should (e.g., no labels are added yet...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec6d3ce",
      "metadata": {
        "id": "6ec6d3ce"
      },
      "source": [
        "To visualize the second-degree network, we use a Python library called nx (more information about other alternatives is here: https://towardsdatascience.com/visualizing-networks-in-python-d70f4cbeb259). Here below we set the source and target for the sub-dataset of the letters generated above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c60380",
      "metadata": {
        "id": "11c60380"
      },
      "outputs": [],
      "source": [
        "# set the data for the visualization according to NX requirements\n",
        "G = nx.from_pandas_edgelist(secDeg_df,\n",
        "                           source='Source',\n",
        "                           target='Target',\n",
        "                           edge_attr='weight')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e04044",
      "metadata": {
        "id": "82e04044"
      },
      "source": [
        "The nx python library has different options to choose from for the type of visualization. Here below we chose \"draw_spring\". More options are shown in the link provided above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b551214",
      "metadata": {
        "id": "6b551214"
      },
      "outputs": [],
      "source": [
        "# visualize the second-degree network of the person of interest\n",
        "nx.draw_kamada_kawai(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a4034b",
      "metadata": {
        "id": "24a4034b"
      },
      "source": [
        "As indicated in the link above (https://towardsdatascience.com/visualizing-networks-in-python-d70f4cbeb259), there are also other libraries that are useful to generate better visualizations. Here below we experiment with pyvis (be patient! is very slow...):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alternative way of plotting with the library matplotlib.pyplot\n",
        "plt.figure(figsize=(20,14))\n",
        "\n",
        "nx.draw(G, pos = nx.nx_pydot.graphviz_layout(G), \\\n",
        "    node_size=200, node_color='lightblue', linewidths=1.5, \\\n",
        "    font_size=9, font_weight='light', with_labels=True)"
      ],
      "metadata": {
        "id": "DiPQr9cONvJF"
      },
      "id": "DiPQr9cONvJF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below shows another type of visualization using the pyvis library (more information in the link provided above). However, it doesn't work properly via Google Colab. If you install this notebook in your computer, it will probably work well. That's why the code is commented."
      ],
      "metadata": {
        "id": "OkHM0shCOWER"
      },
      "id": "OkHM0shCOWER"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d66c53",
      "metadata": {
        "id": "59d66c53"
      },
      "outputs": [],
      "source": [
        "# !pip install pyvis. ##install via the terminal if you have the notebook running in your machine\n",
        "# # import pyvis\n",
        "# from pyvis.network import Network\n",
        "# # create vis network\n",
        "# net = Network(notebook=True)\n",
        "# # load the network graph\n",
        "# net.from_nx(G)\n",
        "# # show\n",
        "# net.show(\"second-degree network of person of interest.html\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90619ad4",
      "metadata": {
        "id": "90619ad4"
      },
      "source": [
        "### Export relations data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc06ea96",
      "metadata": {
        "id": "bc06ea96"
      },
      "source": [
        "Even though the python network visualizations and analysis libraries are very powerful (here not yet used to their maximum possibilities), one may be more familiar with other tools, for example Gephi. Most network visualization/analysis tools need a data input in the format: Source/Target/Weight.\n",
        "Here below we provide the first two columns, and the nodes file, ready to import to Gephi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacaef4b",
      "metadata": {
        "id": "cacaef4b"
      },
      "outputs": [],
      "source": [
        "# First prepare the nodes (i.e., the persons). Gephi requires these labeled as Id,Name\n",
        "nodes_t01 = secondDegree.reset_index(drop=True).copy()\n",
        "nodes_t01.rename(columns={'SKpersonId':'Id', 'personStrId':'Name'},inplace=True)\n",
        "nodes_t02 = nodes_t01.copy()\n",
        "# replacing commas in the name for semicoloms, because the comma will be used as separator\n",
        "nodes_t02['Name'].replace(to_replace=r',', value=';', regex=True).copy()\n",
        "nodes = nodes_t02.reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f01ce80",
      "metadata": {
        "id": "8f01ce80"
      },
      "outputs": [],
      "source": [
        "# Then prepare the edges (i.e., the relations). Gephi requires these labeled as Source,Target\n",
        "edges_t01 = letters_2ndD_unique.reset_index(drop=True).copy()\n",
        "edges_t02 = edges_t01[['SKpersonId_sender', 'SKpersonId_receiver']]\n",
        "edges_t01.rename(columns={'SKpersonId_sender':'Source', 'SKpersonId_receiver':'Receiver'},inplace=True)\n",
        "edges = edges_t01.reset_index(drop=True).copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32628782",
      "metadata": {
        "id": "32628782"
      },
      "source": [
        "Here below are the commands to export the two files ready to upload to Gephi. However, you have to uncomment the code for it to work (we commented it to avoid automatic downloading if you run the entire notebook at once). Be careful to not uncomment the real comments (i.e., those who have two dashes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32916b05",
      "metadata": {
        "id": "32916b05"
      },
      "outputs": [],
      "source": [
        "# add date, version and mapping cycle \n",
        "\n",
        "# this command will add the date to the file name when downloading the nodes\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "fileName01 = (f\"Nodes_{timestr}.csv\")\n",
        "nodes.to_csv(fileName01, index = False)\n",
        "\n",
        "# this command will add the date to the file name when downloading the nodes\n",
        "fileName02= (f\"Edges_{timestr}.csv\")\n",
        "edges.to_csv(fileName02, index = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae02d85",
      "metadata": {
        "id": "0ae02d85"
      },
      "source": [
        "## Curiosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42bc7b69",
      "metadata": {
        "id": "42bc7b69"
      },
      "outputs": [],
      "source": [
        "# earliest correspondents\n",
        "dateBirth_nonZero_df = basicInfo_person_df[basicInfo_person_df['dateBirth'] != 0]\n",
        "earliest_person_df = dateBirth_nonZero_df.sort_values(by='dateBirth', ascending=True)\n",
        "earliest_person_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d700dba4",
      "metadata": {
        "id": "d700dba4"
      },
      "outputs": [],
      "source": [
        "# latest correspondents\n",
        "earliest_person_df.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b45f8df7",
      "metadata": {
        "id": "b45f8df7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "332px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}